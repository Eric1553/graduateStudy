第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)

1.1 训练，验证，测试集（Train / Dev / Test sets）
  李沐的教材中称为Validation set，验证集。
  
  对于很多应用系统，即使是经验丰富的深度学习行家也不太可能一开始就预设出最匹配的超级参数，所以说，应用深度学习是一个典型的迭代过程，需要多次循环往复，才能为应用程序找到一个称心的神经
网络，因此循环该过程的效率是决定项目进展速度的一个关键因素，而创建高质量的训练数据集，验证集和测试集也有助于提高循环效率。

  训练集验证集测试集的划分：3:1:1
  在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照 60%训练，20%验证和 20%测试集来划分。
  
  大数据量的数据集划分：假设我们有 100 万条数据，其中 1 万条作为验证集，1 万条作为测试集，100 万里取 1 万，比例是 1%，即：训练集占 98%，验证集和测试集各占 1%。
  对于数据量过百万的应用，训练集可以占到 99.5%，验证和测试集各占 0.25%，或者验证集占 0.4%，测试集占 0.1%。
  
  Tips：训练集和验证集最好来自同一个分布，效果比较好（比如图片最好是用同一种设备拍摄，清晰度等差不多）
  
  最后一点，就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试
集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。
  
1.2 偏差，方差（Bias /Variance）
  欠拟合、适度拟合、过拟合：
  1.如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（high bias）的情况，我们称为“欠拟合”（underfitting）。
  2.相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式。分类器方差
较高（high variance），数据过度拟合（overfitting）。
  3.在两者之间，可能还有一些像图中这样的，复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（just right）是介于过度拟合和欠拟合中间的一类。
  
  理解：为什么要防止数据的过拟合，过拟合会造成高方差的情况，对于边界上的一些比较难以区分的情况，分类器会产生错误，从而产生验证集和训练集的误差都不高，但是验证集与训练集的误差差距较大。
  采用曲线函数或二次元函数会产生高方差，因为它曲线灵活性太高以致拟合了这两个错误样本和中间这些活跃数据。这看起来有些不自然，从两个维度上看都不太自然，
  但对于高维数据，有些数据区域偏差高，有些数据区域方差高，所以在高维数据中采用这种分类器看起来就不会那么牵强了。
  
  衡量一个训练集和验证集的好坏并不是说比较高的偏差就一定是不好的，需要综合贝叶斯概率（计算出的贝叶斯误差、最优误差）来判断，在最优误差附近的误差都是可以接受的。
  1.训练集误差较贝叶斯误差高许多，验证集误差与训练集相近，即欠拟合情况。
  2.训练集误差较贝叶斯误差差不多，验证集误差与训练集相差较大，即过拟合情况。
  3.训练集误差较贝叶斯误差差不多，验证集误差与训练集相近,即适度拟合情况。
  
  当所有分类器都不适用时，如何分析偏差和方差呢？比如，图片很模糊，即使是人眼，或者没有系统可以准确无误地识别图片，在这种情况下，最优误差会更高，那么分析过程就要做些改变了，
我们暂时先不讨论这些细微差别，重点是通过查看训练集误差，我们可以判断数据拟合情况，至少对于训练数据是这样，可以判断是否有偏差问题，然后查看错误率有多高。当完成训练集训练，
开始使用验证集验证时，我们可以判断方差是否过高，从训练集到验证集的这个过程中，我们可以判断方差是否过高。

  以上分析的前提都是假设基本误差很小，训练集和验证集数据来自相同分布，如果没有这些假设作为前提，分析过程更加复杂。
  
1.3 机器学习基础（Basic Recipe for Machine Learning）
  基本方法：初始模型训练完成后，我首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，
比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法。
  采用规模更大的网络通常都会有所帮助，延长训练时间不一定有用，但也没什么坏处。
  ※※如果网络足够大，通常可以很好的拟合训练集，只要你能扩大网络规模，如果图片很模糊，算法可能无法拟合该图片，但如果有人可以分辨出图片，如果你觉得基本误差不是很高，
那么训练一个更大的网络，你就应该可以……至少可以很好地拟合训练集，至少可以拟合或者过拟合训练集。一旦偏差降低到可以接受的数值，检查一下方差有没有问题，为了评估方
差，我们要查看验证集性能，我们能从一个性能理想的训练集推断出验证集的性能是否也理想，如果方差高，最好的解决办法就是采用更多数据，如果你能做到，会有一定的帮助，但
有时候，我们无法获得更多数据，我们也可以尝试通过正则化来减少过拟合，这个我们下节课会讲。有时候我们不得不反复尝试，但是，如果能找到更合适的神经网络框架，有时它可
能会一箭双雕，同时减少方差和偏差。如何实现呢？想系统地说出做法很难，总之就是不断重复尝试，直到找到一个低偏差，低方差的框架，这时你就成功了。

  注意：
  1.高偏差和高方差是两个完全不同的情况。对于高偏差，准备的数据量与这种情况的联系不大，是分类器或者网络搭建方面的问题。对于高方差，解决的方法最好是准备更多可靠的数据，
或者考虑通过正则化来降低方差。
  2.在机器学习的初期阶段，关于所谓的偏差方差权衡的讨论屡见不鲜，原因是我们能尝试的方法有很多。可以增加偏差，减少方差，也可以减少偏差，增加方差，但是在深
度学习的早期阶段，我们没有太多工具可以做到只减少偏差或方差却不影响到另一方。但在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要准备了更多数据，那么
也并非只有这两种情况，我们假定是这样，那么，只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时
减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。我觉得这就是深度
学习对监督式学习大有裨益的一个重要原因，也是我们不用太过关注如何平衡偏差和方差的一个重要原因，但有时我们有很多选择，减少偏差或方差而不增加另一方。最终，我们会得
到一个非常规范化的网络。从下节课开始，我们将讲解正则化，训练一个更大的网络几乎没有任何负面影响，而训练一个大型神经网络的主要代价也只是计算时间，前提是网络是比较规范化的。

  正则化对于深度学习网络性能的重要性:正则化，它是一种非常实用的减少方差的方法;正则化时会出现偏差方差权衡问题，偏差可能略有增加，如果网络足够大，增幅通常不会太高.
  
1.4 正则化（Regularization）
  深度学习可能存在过拟合问题----高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者
获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。

  L2正则化：
  在逻辑回归函数中加入正则化，只需添加参数 λ，也就是正则化参数。λ/2𝑚乘以𝑤范数的平方，𝑤欧几里德范数的平方等于𝑤𝑗（𝑗 值从 1 到𝑛𝑥）平方的和，也可
表示为𝑤𝑇𝑤，也就是向量参数𝑤 的欧几里德范数（2 范数）的平方，此方法称为𝐿2正则化。因为这里用了欧几里德法线，被称为向量参数𝑤的𝐿2范数。
（w几乎是所有的参数，对于单个参数b可以不用进行正则化）
  神经网络含有一个成本函数，该函数包含𝑊[1]，𝑏[1]到𝑊[𝑙]，𝑏[𝑙]所有参数，字母𝐿是神经网络所含的层数，因此成本函数等于𝑚个训练样本损失函数的总和乘以 1/𝑚，
正则项为𝜆/2𝑚 * ∑ |𝑊[𝑙]| ²，我们称||𝑊[𝑙]||²为范数平方，这个矩阵范数||𝑊[𝑙]||²（即平方范数），被定义为矩阵中所有元素的平方求和。

  L1正则化：𝐿1正则化加的不是𝐿2范数，而是正则项𝜆/2𝑚乘以∑ |𝑤|,∑ |𝑤| 也被称为参数𝑤向量的𝐿1范数，无论分母是𝑚还 是2𝑚，它都是一个比例常量。
  
  L1正则化的弊端：
  如果用的是𝐿1正则化，𝑤最终会是稀疏的，也就是说𝑤向量中有很多 0，有人说这样有利于压缩模型，因为集合中参数均为 0，存储模型所占用的内存更少。实际上，虽然𝐿1正则
化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是𝐿1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用𝐿2正则化。

  L2正则化的计算：我们看下求和公式的具体参数，第一个求和符号其值𝑖从 1 到𝑛[𝑙 − 1]，第二个其𝐽值从 1 到 𝑛[𝑙]，因为𝑊是一个𝑛[𝑙] × 𝑛[𝑙 − 1]的多维矩阵，
𝑛[𝑙]表示𝑙 层单元的数量，𝑛[𝑙−1]表示第𝑙 − 1层隐藏单元的数量。
该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标𝐹标注，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵𝐿2范数”，而称它为“弗罗贝尼乌斯范数”，矩阵𝐿2范数听起来更
自然，但鉴于一些大家无须知道的特殊原因，按照惯例，我们称之为“弗罗贝尼乌斯范数”，它表示一个矩阵中所有元素的平方和。

  该如何使用该范数实现梯度下降呢？
  用 backprop 计算出𝑑𝑊的值，backprop 会给出𝐽对𝑊的偏导数，实际上是𝑊[𝑙]，把𝑊[𝑙]替换为𝑊[𝑙]减去学习率乘以𝑑𝑊。
  既然已经增加了这个正则项，现在我们要做的就是给𝑑𝑊加上这一项𝜆/𝑚 𝑊[𝑙]，然后计算这个更新项，使用新定义的𝑑𝑊[𝑙]，它的定义含有相关
参数代价函数导数和，以及最后添加的额外正则项，这也是𝐿2正则化有时被称为“权重衰减”。（即同样对代价函数中额外增加的弗罗贝尼乌斯范数进行求导）
𝑊[𝑙]的定义被更新为𝑊[𝑙]减去学习率𝑎 乘以 backprop 再加上𝜆/𝑚 𝑊[𝑙]

  该正则项说明，不论𝑊[𝑙]是什么，我们都试图让它变得更小。实际上，相当于我们给矩阵 W 乘以(1 − 𝑎 * 𝜆/𝑚)倍的权重，矩阵𝑊减去𝛼 * 𝜆/𝑚倍的它，也就是用这个系数(1 − 𝑎 * 𝜆/𝑚)乘以矩阵
𝑊，该系数小于1，因此𝐿2范数正则化也被称为“权重衰减”，因为它就像一般的梯度下降， 𝑊被更新为少了𝑎乘以 backprop 输出的最初梯度值，同时𝑊也乘以了这个系数，这个系数小于1，
因此𝐿2正则化也被称为“权重衰减”。

1.5 为什么正则化有利于预防过拟合呢？（ Why regularization reduces overfitting?）
  为什么压缩𝐿2范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？
  直观上理解就是如果正则化𝜆设置得足够大，权重矩阵𝑊被设置为接近于 0 的值，直观理解就是把多隐藏单元的权重设为 0，于是基本上消除了这些隐藏单元的许多影响。如果是
这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。
  但是𝜆会存在一个中间值，于是会有一个接近“Just Right”的中间状态。直观理解就是𝜆增加到足够大，𝑊会接近于 0，实际上是不会发生这种情况的，我们尝
试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网
络的所有隐藏单元依然存在，但是它们的影响变得更小了。神经网络变得更简单了，貌似这样更不容易发生过拟合，因此我不确定这个直觉经验是否有用，不过在编程中执行正则化时，
你实际看到一些方差减少的结果。

  我们再来直观感受一下，正则化为什么可以预防过拟合，假设我们用的是这样的双曲线性激活函数。
  用𝑔(𝑧)表示𝑡𝑎𝑛ℎ(𝑧),那么我们发现，只要𝑧非常小，如果𝑧只涉及少量参数，这里我们利用了双曲正切函数的线性状态，只要𝑧可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。
现在你应该摒弃这个直觉，如果正则化参数 λ 很大，激活函数的参数会相对较小，因为代价函数中的参数变大了，如果𝑊很小，相对来说，𝑧也会很小。

  特别是，如果𝑧的值最终在这个范围内，都是相对较小的值，𝑔(𝑧)大致呈线性，每层几乎都是线性的，和线性回归函数一样。第一节课我们讲过，如果每层都是线性的，那么整个网络就是一个线性网络，
即使是一个非常深的深层网络，因具有线性激活函数的特征，最终我们只能计算线性函数，因此，它不适用于非常复杂的决策，以及过度拟合数据集的非线性决策边界，如同我们在幻灯片中看
到的过度拟合高方差的情况。总结一下，如果正则化参数变得很大，参数𝑊很小，𝑧也会相对变小，此时忽略𝑏的影响，𝑧会相对变小，实际上，𝑧的取值范围很小，这个激活函数，也就是曲线函数𝑡𝑎𝑛ℎ会相对呈线
性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。

  如果你使用的是梯度下降函数，在调试梯度下降时，其中一步就是把代价函数𝐽设计成这样一个函数，在调试梯度下降时，它代表梯度下降的调幅数量。可以看到，代价函数对于
梯度下降的每个调幅都单调递减。如果你实施的是正则化函数，请牢记，𝐽已经有一个全新的定义。如果你用的是原函数𝐽，也就是这第一个项正则化项，你可能看不到单调递减现象，
为了调试梯度下降，请务必使用新定义的𝐽函数，它包含第二个正则化项，否则函数𝐽可能不会在所有调幅范围内都单调递减。

1.6 dropout 正则化（Dropout Regularization）
  除了𝐿2正则化，还有一个非常实用的正则化方法——“Dropout（随机失活）”，我们来看看它的工作原理。
  假设你在训练上图这样的神经网络，它存在过拟合，这就是 dropout 所要处理的，我们复制这个神经网络，dropout 会遍历网络的每一层，并设置消除神经网络中节点的概率。假
设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是 0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后
得到一个节点更少，规模更小的网络，然后用 backprop 方法进行训练。

  如何实施 dropout 呢？方法有几种，接下来我要讲的是最常用的方法，即 inverteddropout（反向随机失活），出于完整性考虑，我们用一个三层（𝑙 = 3）网络来举例说明。
  
  首先要定义向量𝑑，𝑑[3]表示一个三层的 dropout 向量：d3 = np.random.rand(a3.shape[0],a3.shape[1])然后看它是否小于某数，我们称之为 keep-prob，keep-prob 是一个具体数字，
上个示例中它是 0.5，而本例中它是 0.8，它表示保留某个隐藏单元的概率，此处 keep-prob 等于 0.8，它意味着消除任意一个隐藏单元的概率是 0.2，它的作用就是生成随机矩阵，
如果对𝑎[3]进行因子分解，效果也是一样的。𝑑[3]是一个矩阵，每个样本和每个隐藏单元，其中𝑑[3]中的对应值为 1 的概率都是 0.8，对应为 0 的概率是 0.2，随机数字小于 0.8。
它等于 1 的概率是 0.8，等于 0 的概率是 0.2。接下来要做的就是从第三层中获取激活函数，这里我们叫它𝑎[3]，𝑎[3]含有要计算的激活函数，𝑎[3]等于上面的𝑎[3]乘以𝑑[3]，
a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为𝑎3 ∗= 𝑑3，它的作用就是让𝑑[3]中所有等于 0 的元素（输出），而各个元素等于 0 的概率只有 20%，乘法运算最终把𝑑[3]中相应元素输出，即让𝑑[3]中 0 元素与𝑎[3]中相对元素归零。

如果用 python 实现该算法的话，𝑑[3]则是一个布尔型数组，值为 true 和 false，而不是1 和 0，乘法运算依然有效，python 会把 true 和 false 翻译为 1 和 0，大家可以用 python 尝试一下。

最后，我们向外扩展𝑎[3]，用它除以 0.8，或者除以 keep-prob 参数。𝑎3/= 𝑘𝑒𝑒𝑝 − 𝑝𝑟𝑜𝑏
下面我解释一下为什么要这么做，为方便起见，我们假设第三隐藏层上有 50 个单元或50 个神经元，在一维上𝑎[3]是 50，我们通过因子分解将它拆分成50 × 𝑚维的，保留和删除它
们的概率分别为 80%和 20%，这意味着最后被删除或归零的单元平均有 10（50×20%=10）个，现在我们看下𝑧[4]，𝑧[4] = 𝑤[4]𝑎[3] + 𝑏[4]，我们的预期是，𝑎[3]减少 20%，
也就是说𝑎[3]中 有 20%的元素被归零，为了不影响𝑧[4]的期望值，我们需要用𝑤[4]𝑎[3]/0.8，它将会修正或弥补我们所需的那 20%，𝑎[3]的期望值不会变，划线部分就是所谓的 dropout 方法。

据我了解，目前实施 dropout 最常用的方法就是 Inverted dropout。Dropout 早期的迭代版本都没有除以 keep-prob，所以在测试阶段，平均值会变得越来越复杂，不过那些版本已经不再使用了。
  
测试阶段无需使用dropout。

1.7 理解 dropout（Understanding Dropout）
Dropout 可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？

直观上理解：
1.不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout
将产生收缩权重的平方范数的效果，和之前讲的𝐿2正则化类似；实施 dropout 的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；𝐿2对不同权重的衰减是不同的，它取决于激活函数倍增的大小。

2.我们从单个神经元入手，如图，这个单元的工作就是输入并生成一些有意义的输出。通过 dropout，该单元的输入几乎被消除，有时这两个单元会被删除，有时会删除其它单元，
就是说，我用紫色圈起来的这个单元，它不能依靠任何特征，因为特征都有可能被随机清除，或者说该单元的输入也都可能被随机清除。我不愿意把所有赌注都放在一个节点上，
不愿意给任何一个输入加上太多权重，因为它可能会被删除，因此该单元将通过这种方式积极地传播开，并为单元的四个输入增加一点权重，通过传播所有权重，
dropout 将产生收缩权重的平方范数的效果，和我们之前讲过的𝐿2正则化类似，实施 dropout的结果是它会压缩权重，并完成一些预防过拟合的外层正则化。

事实证明，dropout 被正式地作为一种正则化的替代形式，𝐿2对不同权重的衰减是不同的，它取决于倍增的激活函数的大小。
总结一下，dropout 的功能类似于𝐿2正则化，与𝐿2正则化不同的是，被应用的方式不同，dropout 也会有所不同，甚至更适用于不同的输入范围。

在W矩阵比较大的隐藏层，设置比较小的keep-prob，这样可以减小过拟合的情况。在参数比较少的隐藏层则使用比较大的keep-prob值，或者keep-prob = 1，这样会消除比较少或者不消除神经元。
但注意，dropout并不是必须的，只有在出现过拟合的情况时，再考虑使用dropout进行正则化减少过拟合。

1.8 其他正则化方法（Other regularization methods）
一.数据扩增
假设你正在拟合猫咪图片分类器，如果你想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例
如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们
额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。

除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。
通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这
么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。

像这样人工合成数据的话，我们要通过算法验证，图片中的猫经过水平翻转之后依然是猫。大家注意，我并没有垂直翻转，因为我们不想上下颠倒图片，也可以随机选取放大后的部分图片，猫可能还在上面。
对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数
字 4 看起来是波形的，其实不用对数字 4 做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因
为这几个 4 看起来有点扭曲。所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。

二.early stopping
因为在训练过程中，我们希望训练误差，代价函数𝐽都在下降，通过 early stopping，我们不但可以绘制上面这些内容，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，
逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升，early stopping 的作用是神经网络已经在这个迭代过程中表现得很好了，我们在此停止训练得到验证集误差。

当你还未在神经网络上运行太多迭代过程的时候，参数𝑤接近 0，因为随机初始化𝑤值时，它的值可能都是较小的随机值，所以在你长期训练神经网络之前𝑤依然很小，在迭代过
程和训练过程中𝑤的值会变得越来越大，比如在这儿，神经网络中参数𝑤的值已经非常大了，所以 early stopping 要做就是在中间点停止迭代过程，我们得到一个𝑤值中等大小的弗罗贝
尼乌斯范数，与𝐿2正则化相似，选择参数𝑤范数较小的神经网络，但愿你的神经网络过度拟合不严重。

在机器学习中，超级参数激增，选出可行的算法也变得越来越复杂。我发现，如果我们用一组工具优化代价函数𝐽，机器学习就会变得更简单，在重点优化代价函数𝐽时，你只需要
留意𝑤和𝑏，𝐽(𝑤, 𝑏)的值越小越好，你只需要想办法减小这个值，其它的不用关注。然后，预防过拟合还有其他任务，换句话说就是减少方差，这一步我们用另外一套工具来实现，这个
原理有时被称为“正交化”。思路就是在一个时间做一个任务，后面课上我会具体介绍正交化，如果你还不了解这个概念，不用担心。

但对我来说 early stopping 的主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数𝐽，因为现在你不再尝试降低代价函数𝐽，所以代价
函数𝐽的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。

如果不用 early stopping，另一种方法就是𝐿2正则化，训练神经网络的时间就可能很长。我发现，这导致超级参数搜索空间更容易分解，也更容易搜索，但是缺点在于，你必须尝试
很多正则化参数𝜆的值，这也导致搜索大量𝜆值的计算代价太高。
Early stopping 的优点是，只运行一次梯度下降，你可以找出𝑤的较小值，中间值和较大值，而无需尝试𝐿2正则化超级参数𝜆的很多值。

1.9 归一化输入（Normalizing inputs）
训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，
输入特征为 2 维，归一化需要两个步骤：
1.零均值
2.归一化方差

第一步是零均值化，𝜇 = 1/𝑚 ∑ 𝑥(𝑖) ，它是一个向量，𝑥等于每个训练数据 𝑥减去𝜇，意思是移动训练集，直到它完成零均值化

第二步是归一化方差，注意特征𝑥1的方差比特征𝑥2的方差要大得多，我们要做的是给𝜎赋值，𝜎2 = 1/𝑚 ∑ (𝑥(𝑖))²  ，这是节点𝑦 的平方，𝜎2是一个向量，它的每个特征都有方差，
注意，我们已经完成零值均化，(𝑥(𝑖))²元素𝑦²就是方差，我们把所有数据除以向量𝜎2，最后变成上图形式。
𝑥1和𝑥2的方差都等于 1。提示一下，如果你用它来调整训练数据，那么用相同的 𝜇 和 𝜎2来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论𝜇的值是什么，也不论𝜎2的值是什么，
这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估𝜇 和 𝜎2。
因为我们希望不论是训练数据还是测试数据，都是通过相同 μ 和𝜎2定义的相同数据转换，其中𝜇和𝜎2是由训练集数据计算得来的。

要点：使用训练集的均值和方差来统一对训练集和验证集进行归一化，不要在不同的数据集上应用不同的操作。

归一化的作用：数据进行归一化后产生的代价函数取值更加集中（在图中表现为近似的球形或者椭圆形的区域），未归一化样本会导致代价函数比较狭长，从而迭代的效率变慢。

如果输入特征处于不同范围内，可能有些特征值从 0 到 1，有些从 1 到 1000，那么归一化特征值就非常重要了。如果特征值处于相似范围内，那么归一化就不是很重要了。执
行这类归一化并不会产生什么危害，我通常会做归一化处理，虽然我不确定它能否提高训练或算法速度。

1.10 梯度消失/梯度爆炸（Vanishing / Exploding gradients）
训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。

假设我们使用激活函数𝑔(𝑧) = 𝑧，
也就是线性激活函数，我们忽略𝑏，假设𝑏[𝑙]=0，如果那样的话，输出： 𝑦 = 𝑊[𝑙]𝑊[𝐿−1]𝑊[𝐿−2] … 𝑊[3]𝑊[2]𝑊[1]𝑥

权重𝑊只比 1 略大一点，或者说只是比单位矩阵大一点，深度神经网络的激活函数将爆炸式增长。
如果𝑊比 1 略小一点，在深度神经网络中，激活函数将以指数级递减，与层数𝐿相关的导数或梯度函数，也是呈指数级增长或呈指数递减。

1.11 神经网络的权重初始化（Weight Initialization for Deep Networks）
为了预防𝑧值过大或过小，你可以看到𝑛越大，你希望𝑤𝑖越小，因为𝑧是𝑤𝑖𝑥𝑖的和，如果你把很多此类项相加，希望每项值更小，最合理的方法就是设置𝑤𝑖 = 1𝑛，𝑛表示神经元的输入特征数量，
实际上，你要做的就是设置某层权重矩阵𝑤[𝑙] = 𝑛𝑝. 𝑟𝑎𝑛𝑑𝑜𝑚. 𝑟𝑎𝑛𝑑𝑛(shape) ∗ np. sqrt(1 / 𝑛[l−1])，𝑛[𝑙−1]就是我喂给第𝑙层神经单元的数量（即第𝑙 − 1层神经元数量）。
































