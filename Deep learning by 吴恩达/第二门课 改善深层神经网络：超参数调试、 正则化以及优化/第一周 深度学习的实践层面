第二门课 改善深层神经网络：超参数调试、正则化以及优化(Improving Deep Neural Networks:Hyperparameter tuning, Regularization and Optimization)

1.1 训练，验证，测试集（Train / Dev / Test sets）
  李沐的教材中称为Validation set，验证集。
  
  对于很多应用系统，即使是经验丰富的深度学习行家也不太可能一开始就预设出最匹配的超级参数，所以说，应用深度学习是一个典型的迭代过程，需要多次循环往复，才能为应用程序找到一个称心的神经
网络，因此循环该过程的效率是决定项目进展速度的一个关键因素，而创建高质量的训练数据集，验证集和测试集也有助于提高循环效率。

  训练集验证集测试集的划分：3:1:1
  在机器学习发展的小数据量时代，常见做法是将所有数据三七分，就是人们常说的70%验证集，30%测试集，如果没有明确设置验证集，也可以按照 60%训练，20%验证和 20%测试集来划分。
  
  大数据量的数据集划分：假设我们有 100 万条数据，其中 1 万条作为验证集，1 万条作为测试集，100 万里取 1 万，比例是 1%，即：训练集占 98%，验证集和测试集各占 1%。
  对于数据量过百万的应用，训练集可以占到 99.5%，验证和测试集各占 0.25%，或者验证集占 0.4%，测试集占 0.1%。
  
  Tips：训练集和验证集最好来自同一个分布，效果比较好（比如图片最好是用同一种设备拍摄，清晰度等差不多）
  
  最后一点，就算没有测试集也不要紧，测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。所以如果只有验证集，没有测试
集，我们要做的就是，在训练集上训练，尝试不同的模型框架，在验证集上评估这些模型，然后迭代并选出适用的模型。因为验证集中已经涵盖测试集数据，其不再提供无偏性能评估。
  
1.2 偏差，方差（Bias /Variance）
  欠拟合、适度拟合、过拟合：
  1.如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（high bias）的情况，我们称为“欠拟合”（underfitting）。
  2.相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式。分类器方差
较高（high variance），数据过度拟合（overfitting）。
  3.在两者之间，可能还有一些像图中这样的，复杂程度适中，数据拟合适度的分类器，这个数据拟合看起来更加合理，我们称之为“适度拟合”（just right）是介于过度拟合和欠拟合中间的一类。
  
  理解：为什么要防止数据的过拟合，过拟合会造成高方差的情况，对于边界上的一些比较难以区分的情况，分类器会产生错误，从而产生验证集和训练集的误差都不高，但是验证集与训练集的误差差距较大。
  采用曲线函数或二次元函数会产生高方差，因为它曲线灵活性太高以致拟合了这两个错误样本和中间这些活跃数据。这看起来有些不自然，从两个维度上看都不太自然，
  但对于高维数据，有些数据区域偏差高，有些数据区域方差高，所以在高维数据中采用这种分类器看起来就不会那么牵强了。
  
  衡量一个训练集和验证集的好坏并不是说比较高的偏差就一定是不好的，需要综合贝叶斯概率（计算出的贝叶斯误差、最优误差）来判断，在最优误差附近的误差都是可以接受的。
  1.训练集误差较贝叶斯误差高许多，验证集误差与训练集相近，即欠拟合情况。
  2.训练集误差较贝叶斯误差差不多，验证集误差与训练集相差较大，即过拟合情况。
  3.训练集误差较贝叶斯误差差不多，验证集误差与训练集相近,即适度拟合情况。
  
  当所有分类器都不适用时，如何分析偏差和方差呢？比如，图片很模糊，即使是人眼，或者没有系统可以准确无误地识别图片，在这种情况下，最优误差会更高，那么分析过程就要做些改变了，
我们暂时先不讨论这些细微差别，重点是通过查看训练集误差，我们可以判断数据拟合情况，至少对于训练数据是这样，可以判断是否有偏差问题，然后查看错误率有多高。当完成训练集训练，
开始使用验证集验证时，我们可以判断方差是否过高，从训练集到验证集的这个过程中，我们可以判断方差是否过高。

  以上分析的前提都是假设基本误差很小，训练集和验证集数据来自相同分布，如果没有这些假设作为前提，分析过程更加复杂。
  
1.3 机器学习基础（Basic Recipe for Machine Learning）
  基本方法：初始模型训练完成后，我首先要知道算法的偏差高不高，如果偏差较高，试着评估训练集或训练数据的性能。如果偏差的确很高，甚至无法拟合训练集，那么你要做的就是选择一个新的网络，
比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法。
  采用规模更大的网络通常都会有所帮助，延长训练时间不一定有用，但也没什么坏处。
  ※※如果网络足够大，通常可以很好的拟合训练集，只要你能扩大网络规模，如果图片很模糊，算法可能无法拟合该图片，但如果有人可以分辨出图片，如果你觉得基本误差不是很高，
那么训练一个更大的网络，你就应该可以……至少可以很好地拟合训练集，至少可以拟合或者过拟合训练集。一旦偏差降低到可以接受的数值，检查一下方差有没有问题，为了评估方
差，我们要查看验证集性能，我们能从一个性能理想的训练集推断出验证集的性能是否也理想，如果方差高，最好的解决办法就是采用更多数据，如果你能做到，会有一定的帮助，但
有时候，我们无法获得更多数据，我们也可以尝试通过正则化来减少过拟合，这个我们下节课会讲。有时候我们不得不反复尝试，但是，如果能找到更合适的神经网络框架，有时它可
能会一箭双雕，同时减少方差和偏差。如何实现呢？想系统地说出做法很难，总之就是不断重复尝试，直到找到一个低偏差，低方差的框架，这时你就成功了。

  注意：
  1.高偏差和高方差是两个完全不同的情况。对于高偏差，准备的数据量与这种情况的联系不大，是分类器或者网络搭建方面的问题。对于高方差，解决的方法最好是准备更多可靠的数据，
或者考虑通过正则化来降低方差。
  2.在机器学习的初期阶段，关于所谓的偏差方差权衡的讨论屡见不鲜，原因是我们能尝试的方法有很多。可以增加偏差，减少方差，也可以减少偏差，增加方差，但是在深
度学习的早期阶段，我们没有太多工具可以做到只减少偏差或方差却不影响到另一方。但在当前的深度学习和大数据时代，只要持续训练一个更大的网络，只要准备了更多数据，那么
也并非只有这两种情况，我们假定是这样，那么，只要正则适度，通常构建一个更大的网络便可以，在不影响方差的同时减少偏差，而采用更多数据通常可以在不过多影响偏差的同时
减少方差。这两步实际要做的工作是：训练网络，选择网络或者准备更多数据，现在我们有工具可以做到在减少偏差或方差的同时，不对另一方产生过多不良影响。我觉得这就是深度
学习对监督式学习大有裨益的一个重要原因，也是我们不用太过关注如何平衡偏差和方差的一个重要原因，但有时我们有很多选择，减少偏差或方差而不增加另一方。最终，我们会得
到一个非常规范化的网络。从下节课开始，我们将讲解正则化，训练一个更大的网络几乎没有任何负面影响，而训练一个大型神经网络的主要代价也只是计算时间，前提是网络是比较规范化的。

  正则化对于深度学习网络性能的重要性:正则化，它是一种非常实用的减少方差的方法;正则化时会出现偏差方差权衡问题，偏差可能略有增加，如果网络足够大，增幅通常不会太高.
  
1.4 正则化（Regularization）
  深度学习可能存在过拟合问题----高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者
获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。

  L2正则化：
  在逻辑回归函数中加入正则化，只需添加参数 λ，也就是正则化参数。λ/2𝑚乘以𝑤范数的平方，𝑤欧几里德范数的平方等于𝑤𝑗（𝑗 值从 1 到𝑛𝑥）平方的和，也可
表示为𝑤𝑇𝑤，也就是向量参数𝑤 的欧几里德范数（2 范数）的平方，此方法称为𝐿2正则化。因为这里用了欧几里德法线，被称为向量参数𝑤的𝐿2范数。
（w几乎是所有的参数，对于单个参数b可以不用进行正则化）
  神经网络含有一个成本函数，该函数包含𝑊[1]，𝑏[1]到𝑊[𝑙]，𝑏[𝑙]所有参数，字母𝐿是神经网络所含的层数，因此成本函数等于𝑚个训练样本损失函数的总和乘以 1/𝑚，
正则项为𝜆/2𝑚 * ∑ |𝑊[𝑙]| ²，我们称||𝑊[𝑙]||²为范数平方，这个矩阵范数||𝑊[𝑙]||²（即平方范数），被定义为矩阵中所有元素的平方求和。

  L1正则化：𝐿1正则化加的不是𝐿2范数，而是正则项𝜆/2𝑚乘以∑ |𝑤|,∑ |𝑤| 也被称为参数𝑤向量的𝐿1范数，无论分母是𝑚还 是2𝑚，它都是一个比例常量。
  
  L1正则化的弊端：
  如果用的是𝐿1正则化，𝑤最终会是稀疏的，也就是说𝑤向量中有很多 0，有人说这样有利于压缩模型，因为集合中参数均为 0，存储模型所占用的内存更少。实际上，虽然𝐿1正则
化使模型变得稀疏，却没有降低太多存储内存，所以我认为这并不是𝐿1正则化的目的，至少不是为了压缩模型，人们在训练网络时，越来越倾向于使用𝐿2正则化。

  L2正则化的计算：我们看下求和公式的具体参数，第一个求和符号其值𝑖从 1 到𝑛[𝑙 − 1]，第二个其𝐽值从 1 到 𝑛[𝑙]，因为𝑊是一个𝑛[𝑙] × 𝑛[𝑙 − 1]的多维矩阵，
𝑛[𝑙]表示𝑙 层单元的数量，𝑛[𝑙−1]表示第𝑙 − 1层隐藏单元的数量。
该矩阵范数被称作“弗罗贝尼乌斯范数”，用下标𝐹标注，鉴于线性代数中一些神秘晦涩的原因，我们不称之为“矩阵𝐿2范数”，而称它为“弗罗贝尼乌斯范数”，矩阵𝐿2范数听起来更
自然，但鉴于一些大家无须知道的特殊原因，按照惯例，我们称之为“弗罗贝尼乌斯范数”，它表示一个矩阵中所有元素的平方和。

  该如何使用该范数实现梯度下降呢？
  用 backprop 计算出𝑑𝑊的值，backprop 会给出𝐽对𝑊的偏导数，实际上是𝑊[𝑙]，把𝑊[𝑙]替换为𝑊[𝑙]减去学习率乘以𝑑𝑊。
  既然已经增加了这个正则项，现在我们要做的就是给𝑑𝑊加上这一项𝜆/𝑚 𝑊[𝑙]，然后计算这个更新项，使用新定义的𝑑𝑊[𝑙]，它的定义含有相关
参数代价函数导数和，以及最后添加的额外正则项，这也是𝐿2正则化有时被称为“权重衰减”。（即同样对代价函数中额外增加的弗罗贝尼乌斯范数进行求导）
𝑊[𝑙]的定义被更新为𝑊[𝑙]减去学习率𝑎 乘以 backprop 再加上𝜆/𝑚 𝑊[𝑙]

  该正则项说明，不论𝑊[𝑙]是什么，我们都试图让它变得更小。实际上，相当于我们给矩阵 W 乘以(1 − 𝑎 * 𝜆/𝑚)倍的权重，矩阵𝑊减去𝛼 * 𝜆/𝑚倍的它，也就是用这个系数(1 − 𝑎 * 𝜆/𝑚)乘以矩阵
𝑊，该系数小于1，因此𝐿2范数正则化也被称为“权重衰减”，因为它就像一般的梯度下降， 𝑊被更新为少了𝑎乘以 backprop 输出的最初梯度值，同时𝑊也乘以了这个系数，这个系数小于1，
因此𝐿2正则化也被称为“权重衰减”。

1.5 为什么正则化有利于预防过拟合呢？（ Why regularization reduces overfitting?）
  为什么压缩𝐿2范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？
  直观上理解就是如果正则化𝜆设置得足够大，权重矩阵𝑊被设置为接近于 0 的值，直观理解就是把多隐藏单元的权重设为 0，于是基本上消除了这些隐藏单元的许多影响。如果是
这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。
  但是𝜆会存在一个中间值，于是会有一个接近“Just Right”的中间状态。直观理解就是𝜆增加到足够大，𝑊会接近于 0，实际上是不会发生这种情况的，我们尝
试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网
络的所有隐藏单元依然存在，但是它们的影响变得更小了。神经网络变得更简单了，貌似这样更不容易发生过拟合，因此我不确定这个直觉经验是否有用，不过在编程中执行正则化时，
你实际看到一些方差减少的结果。

  我们再来直观感受一下，正则化为什么可以预防过拟合，假设我们用的是这样的双曲线性激活函数。
  用𝑔(𝑧)表示𝑡𝑎𝑛ℎ(𝑧),那么我们发现，只要𝑧非常小，如果𝑧只涉及少量参数，这里我们利用了双曲正切函数的线性状态，只要𝑧可以扩展为这样的更大值或者更小值，激活函数开始变得非线性。
现在你应该摒弃这个直觉，如果正则化参数 λ 很大，激活函数的参数会相对较小，因为代价函数中的参数变大了，如果𝑊很小，相对来说，𝑧也会很小。

  特别是，如果𝑧的值最终在这个范围内，都是相对较小的值，𝑔(𝑧)大致呈线性，每层几乎都是线性的，和线性回归函数一样。第一节课我们讲过，如果每层都是线性的，那么整个网络就是一个线性网络，
即使是一个非常深的深层网络，因具有线性激活函数的特征，最终我们只能计算线性函数，因此，它不适用于非常复杂的决策，以及过度拟合数据集的非线性决策边界，如同我们在幻灯片中看
到的过度拟合高方差的情况。总结一下，如果正则化参数变得很大，参数𝑊很小，𝑧也会相对变小，此时忽略𝑏的影响，𝑧会相对变小，实际上，𝑧的取值范围很小，这个激活函数，也就是曲线函数𝑡𝑎𝑛ℎ会相对呈线
性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。

  如果你使用的是梯度下降函数，在调试梯度下降时，其中一步就是把代价函数𝐽设计成这样一个函数，在调试梯度下降时，它代表梯度下降的调幅数量。可以看到，代价函数对于
梯度下降的每个调幅都单调递减。如果你实施的是正则化函数，请牢记，𝐽已经有一个全新的定义。如果你用的是原函数𝐽，也就是这第一个项正则化项，你可能看不到单调递减现象，
为了调试梯度下降，请务必使用新定义的𝐽函数，它包含第二个正则化项，否则函数𝐽可能不会在所有调幅范围内都单调递减。

1.6 dropout 正则化（Dropout Regularization）
  除了𝐿2正则化，还有一个非常实用的正则化方法——“Dropout（随机失活）”，我们来看看它的工作原理。
  假设你在训练上图这样的神经网络，它存在过拟合，这就是 dropout 所要处理的，我们复制这个神经网络，dropout 会遍历网络的每一层，并设置消除神经网络中节点的概率。假
设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是 0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后
得到一个节点更少，规模更小的网络，然后用 backprop 方法进行训练。




  

























