2.1 Mini-batch 梯度下降（Mini-batch gradient descent）
为什么要使用mini-batch分割数据集？数据集中的数据文件太多，如果一次性遍历完所有的数据效率太低，对数据集进行分隔可以有效地加快我们的训练效率。

batch 梯度下降法指的是我们之前讲过的梯度下降法算法，就是同时处理整个训练集，这个名字就是来源于能够同时看到整个 batch 训练集的样本被处理，这个名字不怎么样，但就是这样叫它。
相比之下，mini-batch 梯度下降法，每次同时处理的单个的 mini-batch 𝑋{𝑡}和𝑌{𝑡}，而不是同时处理全部的𝑋和𝑌训练集。

mini-batch:把训练集分割为小一点的子集训练，这些子集被取名为 mini-batch，假设每一个子集中只有 1000 个样本，那么把其中的𝑥(1)到𝑥(1000)取出来，将其称为第一个子训练集，也
叫做 mini-batch，然后你再取出接下来的 1000 个样本，从𝑥(1001)到𝑥(2000)，然后再取 1000个样本。
𝑋{𝑡}和𝑌{𝑡}的维数：如果𝑋{1}是一个有 1000 个样本的训练集，或者说是 1000 个样本的𝑥值，所以维数应该是(𝑛𝑥, 1000)，𝑋{2}的维数应该是(𝑛𝑥, 1000)，以此类推。因此所有的子集
维数都是(𝑛𝑥, 1000)，而这些（𝑌{𝑡}）的维数都是(1,1000)

mini-batch的数量𝑡组成了𝑋{𝑡}和𝑌{𝑡}，其中每个X{t}和Y{t}拥有相同的列数（因为对于每个子集来说，样本数都是不变的）
符号的定义 （i）表示第i个样本，[l]表示第l层的数据，{t}表示第t个mini-batch的数据

在训练集上运行 mini-batch 梯度下降法，你运行 for t=1……5000，因为我们有 5000 个各有 1000 个样本的组，在 for 循环里你要做得基本就是对𝑋{𝑡}和𝑌{𝑡}执行一步梯度下降法。
假设你有一个拥有1000个样本的训练集，而且假设你已经很熟悉一次性处理完的方法，你要用向量化去几乎同时处理 1000 个样本。
首先对输入也就是𝑋{𝑡}，执行前向传播，然后执行𝑧[1] = 𝑊[1]𝑋 + 𝑏[1]，之前我们这里只有，但是现在你正在处理整个训练集，你在处理第一个 mini-batch，在处理 mini-batch 时它
变成了𝑋{𝑡}，即𝑧[1] = 𝑊[1]𝑋{𝑡} + 𝑏[1]，然后执行𝐴[1]𝑘 = 𝑔[1](𝑍[1])，之所以用大写的𝑍是因为这是一个向量内涵，以此类推，直到𝐴[𝐿] = 𝑔[𝐿](𝑍[𝐿])，这就是你的预测值。注意这里你需要
用到一个向量化的执行命令，这个向量化的执行命令，一次性处理 1000 个而不是 500 万个样本。
使用 batch 梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用 mini-batch 梯度下降法，一次遍历训练集，能让你做 5000 个梯度下降。

即，mini-batch的引入减少了每次运行处理的数据数，增加了处理的效率，其他的步骤都和梯度下降一样处理。 
此时计算成本函数的m样本量就发生变化，不再是全部的样本量了，因此参数m需要进行调整，同时正则化中的参数也需要进行修改。

epoch：对所有数据进行迭代一次
batch size：每次送入的数据样本数
iteration：完成一个epoch迭代的次数，相当于epoch / batch size

2.2 理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent）
使用batch梯度下降法的明显特点是，每代的迭代都会使成本函数下降，如果没有发现下降反而上升，可能就是学习率设置的比较高，导致没有获取到最优的成本函数。
使用mini-batch梯度下降法，可能不会每次迭代都会使成本函数得到优化，这是因为每次送进去的样本都相当于一个全新的训练集，部分训练集可能比较容易计算，而某些训练集会比较难以计算，
这种不确定性导致mini-batch不能每次都产生成本函数的下降，但是总的趋势应该是下降的。

mini-batch大小的设定：
如果设置成1，就相当于随机梯度下降法（SGD stichastic gradient descent）sgd需要最多次数的迭代，而且其效率不高。
如果设置成整个训练集大小m，就相当于batch随机梯度下降，每次送进去的数据就是整个训练集，这样的缺点是数据量太大，运行的效率太低。

sgd永远不会收敛，会在最小值的附近进行波动，产生的噪声也比较多，某一个样本的错误可能会使sgd偏离正确的方向。通过减小学习率可以改善噪声比较大的问题，但是缺少了向量化的加速，效率比较低。
batch梯度下降的下降幅度比较大，噪声低，可以不断地寻找最小值。缺点是运行的时间太长，一次迭代的速率太慢。

所以实践中最好选择不大不小的 mini-batch 尺寸，实际上学习率达到最快。你会发现两个好处，一方面，你得到了大量向量化，上个视频中我们用过的例子中，如果 mini-batch 大小为
1000 个样本，你就可以对 1000 个样本向量化，比你一次性处理多个样本快得多。另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作，再用一下上个视频的数字，每
次训练集允许我们采取 5000 个梯度下降步骤，所以实际上一些位于中间的 mini-batch 大小效果最好。

用 mini-batch 梯度下降法，我们从这里开始，一次迭代这样做，两次，三次，四次，它不会总朝向最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向，它也不一定在
很小的范围内收敛或者波动，如果出现这个问题，可以慢慢减少学习率。

首先，如果训练集较小，直接使用 batch 梯度下降法，样本集较小就没必要使用 mini-batch 梯度下降法，你可以快速处理整个训练集，所以使用 batch 梯度下降法也很好，这里
的少是说小于 2000 个样本，这样比较适合使用 batch 梯度下降法。不然，样本数目较大的话，一般的 mini-batch 大小为 64 到 512，考虑到电脑内存设置和使用的方式，
如果 mini-batch 大小是 2 的𝑛次方，代码会运行地快一些，64 就是 2 的 6 次方，以此类推，128 是 2 的 7 次方，256 是 2 的 8 次方，512 是 2 的 9 次方。
所以我经常把 mini-batch 大小设成 2 的次方。在上一个视频里，我的 mini-batch 大小设为了 1000，建议你可以试一下 1024，也就是2 的 10 次方。
也有 mini-batch 的大小为 1024，不过比较少见，64 到 512 的 mini-batch 比较常见。

最后需要注意的是在你的 mini-batch 中，要确保𝑋{𝑡}和𝑌{𝑡}要符合 CPU/GPU 内存，取决于你的应用方向以及训练集的大小。如果你处理的 mini-batch 和 CPU/GPU 内存不相符，不
管你用什么方法处理数据，你会注意到算法的表现急转直下变得惨不忍睹，所以我希望你对一般人们使用的 mini-batch 大小有一个直观了解。事实上 mini-batch 大小是另一个重要的变
量，你需要做一个快速尝试，才能找到能够最有效地减少成本函数的那个，我一般会尝试几个不同的值，几个不同的 2 次方，然后看能否找到一个让梯度下降优化算法最高效的大小。
希望这些能够指导你如何开始找到这一数值。


























