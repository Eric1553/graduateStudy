2.1 Mini-batch 梯度下降（Mini-batch gradient descent）
为什么要使用mini-batch分割数据集？数据集中的数据文件太多，如果一次性遍历完所有的数据效率太低，对数据集进行分隔可以有效地加快我们的训练效率。

mini-batch:把训练集分割为小一点的子集训练，这些子集被取名为 mini-batch，假设每一个子集中只有 1000 个样本，那么把其中的𝑥(1)到𝑥(1000)取出来，将其称为第一个子训练集，也
叫做 mini-batch，然后你再取出接下来的 1000 个样本，从𝑥(1001)到𝑥(2000)，然后再取 1000个样本。
𝑋{𝑡}和𝑌{𝑡}的维数：如果𝑋{1}是一个有 1000 个样本的训练集，或者说是 1000 个样本的𝑥值，所以维数应该是(𝑛𝑥, 1000)，𝑋{2}的维数应该是(𝑛𝑥, 1000)，以此类推。因此所有的子集
维数都是(𝑛𝑥, 1000)，而这些（𝑌{𝑡}）的维数都是(1,1000)

mini-batch的数量𝑡组成了𝑋{𝑡}和𝑌{𝑡}，其中每个X{t}和Y{t}拥有相同的列数（因为对于每个子集来说，样本数都是不变的）
符号的定义 （i）表示第i个样本，[l]表示第l层的数据，{t}表示第t个mini-batch的数据

在训练集上运行 mini-batch 梯度下降法，你运行 for t=1……5000，因为我们有 5000 个各有 1000 个样本的组，在 for 循环里你要做得基本就是对𝑋{𝑡}和𝑌{𝑡}执行一步梯度下降法。
假设你有一个拥有1000个样本的训练集，而且假设你已经很熟悉一次性处理完的方法，你要用向量化去几乎同时处理 1000 个样本。
首先对输入也就是𝑋{𝑡}，执行前向传播，然后执行𝑧[1] = 𝑊[1]𝑋 + 𝑏[1]，之前我们这里只有，但是现在你正在处理整个训练集，你在处理第一个 mini-batch，在处理 mini-batch 时它
变成了𝑋{𝑡}，即𝑧[1] = 𝑊[1]𝑋{𝑡} + 𝑏[1]，然后执行𝐴[1]𝑘 = 𝑔[1](𝑍[1])，之所以用大写的𝑍是因为这是一个向量内涵，以此类推，直到𝐴[𝐿] = 𝑔[𝐿](𝑍[𝐿])，这就是你的预测值。注意这里你需要
用到一个向量化的执行命令，这个向量化的执行命令，一次性处理 1000 个而不是 500 万个样本。

即，mini-batch的引入减少了每次运行处理的数据数，增加了处理的效率，其他的步骤都和梯度下降一样处理。 
