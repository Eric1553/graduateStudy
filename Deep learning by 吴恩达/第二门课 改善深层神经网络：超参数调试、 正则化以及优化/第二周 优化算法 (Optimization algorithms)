2.1 Mini-batch 梯度下降（Mini-batch gradient descent）
为什么要使用mini-batch分割数据集？数据集中的数据文件太多，如果一次性遍历完所有的数据效率太低，对数据集进行分隔可以有效地加快我们的训练效率。

batch 梯度下降法指的是我们之前讲过的梯度下降法算法，就是同时处理整个训练集，这个名字就是来源于能够同时看到整个 batch 训练集的样本被处理，这个名字不怎么样，但就是这样叫它。
相比之下，mini-batch 梯度下降法，每次同时处理的单个的 mini-batch 𝑋{𝑡}和𝑌{𝑡}，而不是同时处理全部的𝑋和𝑌训练集。

mini-batch:把训练集分割为小一点的子集训练，这些子集被取名为 mini-batch，假设每一个子集中只有 1000 个样本，那么把其中的𝑥(1)到𝑥(1000)取出来，将其称为第一个子训练集，也
叫做 mini-batch，然后你再取出接下来的 1000 个样本，从𝑥(1001)到𝑥(2000)，然后再取 1000个样本。
𝑋{𝑡}和𝑌{𝑡}的维数：如果𝑋{1}是一个有 1000 个样本的训练集，或者说是 1000 个样本的𝑥值，所以维数应该是(𝑛𝑥, 1000)，𝑋{2}的维数应该是(𝑛𝑥, 1000)，以此类推。因此所有的子集
维数都是(𝑛𝑥, 1000)，而这些（𝑌{𝑡}）的维数都是(1,1000)

mini-batch的数量𝑡组成了𝑋{𝑡}和𝑌{𝑡}，其中每个X{t}和Y{t}拥有相同的列数（因为对于每个子集来说，样本数都是不变的）
符号的定义 （i）表示第i个样本，[l]表示第l层的数据，{t}表示第t个mini-batch的数据

在训练集上运行 mini-batch 梯度下降法，你运行 for t=1……5000，因为我们有 5000 个各有 1000 个样本的组，在 for 循环里你要做得基本就是对𝑋{𝑡}和𝑌{𝑡}执行一步梯度下降法。
假设你有一个拥有1000个样本的训练集，而且假设你已经很熟悉一次性处理完的方法，你要用向量化去几乎同时处理 1000 个样本。
首先对输入也就是𝑋{𝑡}，执行前向传播，然后执行𝑧[1] = 𝑊[1]𝑋 + 𝑏[1]，之前我们这里只有，但是现在你正在处理整个训练集，你在处理第一个 mini-batch，在处理 mini-batch 时它
变成了𝑋{𝑡}，即𝑧[1] = 𝑊[1]𝑋{𝑡} + 𝑏[1]，然后执行𝐴[1]𝑘 = 𝑔[1](𝑍[1])，之所以用大写的𝑍是因为这是一个向量内涵，以此类推，直到𝐴[𝐿] = 𝑔[𝐿](𝑍[𝐿])，这就是你的预测值。注意这里你需要
用到一个向量化的执行命令，这个向量化的执行命令，一次性处理 1000 个而不是 500 万个样本。
使用 batch 梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用 mini-batch 梯度下降法，一次遍历训练集，能让你做 5000 个梯度下降。

即，mini-batch的引入减少了每次运行处理的数据数，增加了处理的效率，其他的步骤都和梯度下降一样处理。 
此时计算成本函数的m样本量就发生变化，不再是全部的样本量了，因此参数m需要进行调整，同时正则化中的参数也需要进行修改。

epoch：对所有数据进行迭代一次
batch size：每次送入的数据样本数
iteration：完成一个epoch迭代的次数，相当于epoch / batch size

2.2 理解 mini-batch 梯度下降法（Understanding mini-batch gradient descent）
使用batch梯度下降法的明显特点是，每代的迭代都会使成本函数下降，如果没有发现下降反而上升，可能就是学习率设置的比较高，导致没有获取到最优的成本函数。
使用mini-batch梯度下降法，可能不会每次迭代都会使成本函数得到优化，这是因为每次送进去的样本都相当于一个全新的训练集，部分训练集可能比较容易计算，而某些训练集会比较难以计算，
这种不确定性导致mini-batch不能每次都产生成本函数的下降，但是总的趋势应该是下降的。

mini-batch大小的设定：
如果设置成1，就相当于随机梯度下降法（SGD stichastic gradient descent）sgd需要最多次数的迭代，而且其效率不高。
如果设置成整个训练集大小m，就相当于batch随机梯度下降，每次送进去的数据就是整个训练集，这样的缺点是数据量太大，运行的效率太低。

sgd永远不会收敛，会在最小值的附近进行波动，产生的噪声也比较多，某一个样本的错误可能会使sgd偏离正确的方向。通过减小学习率可以改善噪声比较大的问题，但是缺少了向量化的加速，效率比较低。
batch梯度下降的下降幅度比较大，噪声低，可以不断地寻找最小值。缺点是运行的时间太长，一次迭代的速率太慢。

所以实践中最好选择不大不小的 mini-batch 尺寸，实际上学习率达到最快。你会发现两个好处，一方面，你得到了大量向量化，上个视频中我们用过的例子中，如果 mini-batch 大小为
1000 个样本，你就可以对 1000 个样本向量化，比你一次性处理多个样本快得多。另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作，再用一下上个视频的数字，每
次训练集允许我们采取 5000 个梯度下降步骤，所以实际上一些位于中间的 mini-batch 大小效果最好。

用 mini-batch 梯度下降法，我们从这里开始，一次迭代这样做，两次，三次，四次，它不会总朝向最小值靠近，但它比随机梯度下降要更持续地靠近最小值的方向，它也不一定在
很小的范围内收敛或者波动，如果出现这个问题，可以慢慢减少学习率。

首先，如果训练集较小，直接使用 batch 梯度下降法，样本集较小就没必要使用 mini-batch 梯度下降法，你可以快速处理整个训练集，所以使用 batch 梯度下降法也很好，这里
的少是说小于 2000 个样本，这样比较适合使用 batch 梯度下降法。不然，样本数目较大的话，一般的 mini-batch 大小为 64 到 512，考虑到电脑内存设置和使用的方式，
如果 mini-batch 大小是 2 的𝑛次方，代码会运行地快一些，64 就是 2 的 6 次方，以此类推，128 是 2 的 7 次方，256 是 2 的 8 次方，512 是 2 的 9 次方。
所以我经常把 mini-batch 大小设成 2 的次方。在上一个视频里，我的 mini-batch 大小设为了 1000，建议你可以试一下 1024，也就是2 的 10 次方。
也有 mini-batch 的大小为 1024，不过比较少见，64 到 512 的 mini-batch 比较常见。

最后需要注意的是在你的 mini-batch 中，要确保𝑋{𝑡}和𝑌{𝑡}要符合 CPU/GPU 内存，取决于你的应用方向以及训练集的大小。如果你处理的 mini-batch 和 CPU/GPU 内存不相符，不
管你用什么方法处理数据，你会注意到算法的表现急转直下变得惨不忍睹，所以我希望你对一般人们使用的 mini-batch 大小有一个直观了解。事实上 mini-batch 大小是另一个重要的变
量，你需要做一个快速尝试，才能找到能够最有效地减少成本函数的那个，我一般会尝试几个不同的值，几个不同的 2 次方，然后看能否找到一个让梯度下降优化算法最高效的大小。
希望这些能够指导你如何开始找到这一数值。

2.3 指数加权平均数（Exponentially weighted averages）
程序中的移动平均值 𝑣𝑡 = 𝛽 𝑣t-1 + (1 − 𝛽)𝜃𝑡  即 温度的局部平均值（移动平均值） = 前一天温度的β倍 + 当日温度的（1 - β）倍
假设这里的β是0.9，即前一天的温度的移动平均值的占比为0.9，当日的温度占当日温度移动平均值的0.1。此时可以视当日温度的移动平均值为10天的平均温度，vt相当于1 / (1 - β)的每日温度
如果β的取值是0.98，那么就相当于50天温度的平均值。
这里的理解：展开vt-1我们可以得到前一天温度的1-β倍加上大前天温度的移动平均值，由于β是一个比较接近于1的数，因此，之前的每日温度的1-β倍都被比较好的保留了下来，最后的某天温度的移动平均值
就变成了一个等比数列性质的每日温度的1-β倍的和，如果忽略这个等比数列的公比（因为接近于1），我们可以得到每天温度所占的比例为1/(1-β)，相当于1/(1-β)天的每日温度的平均值。

这个高值𝛽要注意几点，你得到的曲线要平坦一些，原因在于你多平均了几天的温度，所以这个曲线，波动更小，更加平坦，缺点是曲线进一步右移，因为现在平均的温度值更多，
要平均更多的值，指数加权平均公式在温度变化时，适应地更缓慢一些，所以会出现一定延迟，因为当𝛽 = 0.98，相当于给前一天的值加了太多权重，只有 0.02 的权重给了当日的值，
所以温度变化时，温度上下起伏，当𝛽 较大时，指数加权平均值适应地更缓慢一些。

因此，适当的选择β的值是选用指数加权平均数时需要重点考虑的问题，要权衡低数值β高噪声高反应灵敏度与高数值β低噪声地反应灵敏度。

2.4 理解指数加权平均数（ Understanding exponentially weighted averages）
指数加权平均数的基本公式：  𝑣𝑡 = 𝛽𝑣𝑡−1 + (1 − 𝛽)𝜃𝑡
展开形式： 𝑣100 = 0.1 𝜃100 + 0.1 × 0.9 𝜃99 + 0.1 × (0.9)² 𝜃98 + 0.1 × (0.9)三次方 𝜃97 + 0.1 × (0.9)四次方 𝜃96 + ⋯

过所有的这些系数（0.10.1 × 0.90.1 × (0.9)2 0.1 × (0.9)3 …），相加起来为 1 或者逼近 1，我们称之为偏差修正

到底需要平均多少天的温度?
实际上(0.9)的10次方大约为 0.35，这大约是1/𝑒， e 是自然算法的基础之一。大体上说，如果有1 − 𝜀，在这个例子中，𝜀 = 0.1，所以1 − 𝜀 = 0.9，(1 − 𝜀)的(1/𝜀)次方约等于1/𝑒，
大约是 0.34，0.35，换句话说，10 天后，曲线的高度下降到1/3，相当于在峰值的1 /𝑒。

这里用到了重要极限 (1 + n)的1/n次方 ≈ 1/e （变形） 因此当β取值为0.98的时候 需要平均50天的数据才能达到1/e的水平

现在解释一下算法，可以将𝑣0，𝑣1，𝑣2等等写成明确的变量，不过在实际中执行的话，你要做的是，一开始将𝑣初始化为 0，然后在第一天使𝑣: = 𝛽𝑣 + (1 − 𝛽)𝜃1，然后第二天，更
新𝑣值，𝑣: = 𝛽𝑣 + (1 − 𝛽)𝜃2，以此类推，有些人会把𝑣加下标，来表示𝑣是用来计算数据的指数加权平均数。
𝑣𝜃 = 0，然后每一天，拿到第𝑡天的数据，把𝑣更新为𝑣: = 𝛽 𝑣 sub(𝜃) + (1 − 𝛽)𝜃t

指数加权平均数公式的好处之一在于，它占用极少内存，电脑内存中只占用一行数字而已，然后把最新数据代入公式，不断覆盖就可以了，正因为这个原因，其效率，它基本上只
占用一行代码，计算指数加权平均数也只占用单行数字的存储和内存，当然它并不是最好的，也不是最精准的计算平均数的方法。如果你要计算移动窗，你直接算出过去 10 天的总和，
过去 50 天的总和，除以 10 和 50 就好，如此往往会得到更好的估测。但缺点是，如果保存所有最近的温度数据，和过去 10 天的总和，必须占用更多的内存，执行更加复杂，计算成
本也更加高昂。

2.5 指数加权平均的偏差修正 （Bias correction in exponentially weighted averages）
计算移动平均数的时候，初始化𝑣0 = 0，𝑣1 = 0.98𝑣0 + 0.02𝜃1，但是𝑣0 = 0，所以这部分没有了（0.98𝑣0），所以𝑣1 = 0.02𝜃1，所以如果一天温度是 40 华氏度，
那么𝑣1 = 0.02𝜃1 = 0.02 × 40 = 8，因此得到的值会小很多，所以第一天温度的估测不准。
𝑣2 = 0.98 𝑣1 + 0.02 𝜃2，如果代入𝑣1，然后相乘，所以𝑣2 = 0.98 × 0.02𝜃1 + 0.02𝜃2 = 0.0196𝜃1 + 0.02𝜃2，假设𝜃1和𝜃2都是正数，计算后𝑣2要远小于𝜃1和𝜃2，所以𝑣2不能很好估测
出这一年前两天的温度。

有个办法可以修改这一估测，让估测变得更好，更准确，特别是在估测初期，也就是不用𝑣𝑡，而是用 𝑣𝑡/1−𝛽的𝑡次方，t 就是现在的天数。举个具体例子，当𝑡 = 2时，1 − 𝛽的𝑡次方 = 1 − 0.982 = 0.0396，
因此对第二天温度的估测变成了 𝑣2 / 0.0396 = 0.0196𝜃1+0.02𝜃2 / 0.0396 ，也就是𝜃1和𝜃2的加权平均数，并去除了偏差。你会发现随着𝑡增加，𝛽𝑡接近于 0，所以当𝑡很大的时候，偏差修正几乎没有作用，
因此当𝑡较大的时候，紫线基本和绿线重合了。不过在开始学习阶段，你才开始预测热身练习，偏差修正可以帮助你更好预测温度，偏差修正可以帮助你使结果从紫线变成绿线。

2.6 动量梯度下降法（Gradient descent with Momentum）
还有一种算法叫做 Momentum，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法
动量梯度下降法用来解决sgd，batch，mini-batch下降法的弊端，这些方法纵轴的摆动比较大,容易偏离函数的取值范围（需要使用较小的学习率）。
因此，动量梯度下降法可以降低纵轴方向的均值，同时横轴方向运动更快.

方法： 𝑣 sub（𝑑𝑊） = 𝛽 * 𝑣 sub（𝑑𝑊） +(1 − 𝛽)𝑑𝑊，这跟我们之前的计算相似，也就是𝑣 = 𝛽𝑣 + (1 − 𝛽)𝜃sub(𝑡)，𝑑𝑊的移动平均数，接着同样地计算𝑣 sub（𝑑𝑏），
𝑣 sub（𝑑𝑏） = 𝛽 𝑣 sub(𝑑𝑏) + (1 − 𝛽)𝑑𝑏，然后重新赋值权重，𝑊: = 𝑊 − 𝑎 𝑣sub(𝑑𝑊)，同样𝑏: = 𝑏 − 𝑎 𝑣sub(𝑑𝑏)，这样就可以减缓梯度下降的幅度。

有两个超参数，学习率𝑎以及参数𝛽，𝛽控制着指数加权平均数。𝛽最常用的值是0.9，我们之前平均了过去十天的温度，所以现在平均了前十次迭代的梯度。实际上𝛽为 0.9
时，效果不错，你可以尝试不同的值，可以做一些超参数的研究，不过 0.9 是很棒的鲁棒数。那么关于偏差修正，所以你要拿𝑣 sub(𝑑𝑊)和𝑣 sub(𝑑𝑏)除以1 − 𝛽的𝑡次方，实际上人们不这么做，
因为 10 次迭代之后，因为你的移动平均已经过了初始阶段。实际中，在使用梯度下降法或动量梯度下降法时，人们不会受到偏差修正的困扰。当然𝑣 sub(𝑑𝑊)初始值是 0，要注意到这是和𝑑𝑊拥有相同维
数的零矩阵，也就是跟𝑊拥有相同的维数，𝑣 sub(𝑑𝑏)的初始值也是向量零，所以和𝑑𝑏拥有相同的维数，也就是和𝑏是同一维数.
























