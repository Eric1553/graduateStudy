3.1 调试处理（Tuning process）
关于训练深度最难的事情之一是你要处理的参数的数量，从学习速率𝑎到 Momentum（动量梯度下降法）的参数𝛽。如果使用 Momentum 或 Adam 优化算法的参数，𝛽1，𝛽2和𝜀，
也许你还得选择层数，也许你还得选择不同层中隐藏单元的数量，也许你还想使用学习率衰减。所以，你使用的不是单一的学习率𝑎。接着，当然你可能还需要选择 mini-batch 的大小。

最重要的超参数：a 学习率

除了𝑎，还有一些参数需要调试，例如 Momentum 参数𝛽，0.9 就是个很好的默认值。我还会调试 mini-batch 的大小，以确保最优算法运行有效。我还会经常调试隐藏单元，我用橙
色圈住的这些，这三个是我觉得其次比较重要的，相对于𝑎而言。重要性排第三位的是其他因素，层数有时会产生很大的影响，学习率衰减也是如此。当应用 Adam 算法时，事实上，
我从不调试𝛽1，𝛽2和𝜀，我总是选定其分别为 0.9，0.999 和10的−8，如果你想的话也可以调试。

Andrew NG认为的超参数的调整重要性：
a >> β = hidden units = mini-batch size > layers = learning rate decay >> β1（默认0.9） = β2（默认0.999） = ε（默认10的-8次方）

方法一：在超参数比较少的情况下，可以使用随机取样本点的方法尝试超参数的学习效果
现在，如果你尝试调整一些超参数，该如何选择调试值呢？在早一代的机器学习算法中，如果你有两个超参数，这里我会称之为超参 1，超参 2，常见的做法是在网格中取样点，像
这样，然后系统的研究这些数值。这里我放置的是 5×5 的网格，实践证明，网格可以是 5×5，也可多可少，但对于这个例子，你可以尝试这所有的 25 个点，然后选择哪个参数效果最好。
当参数的数量相对较少时，这个方法很实用。  

推荐你采用下面的做法，随机选择点，所以你可以选择同等数量的点。25 个点，接着，用这些随机取的点试验超参数的效果。之所以这么做是因为，对于你要解决的问题而言，
你很难提前知道哪个超参数最重要，正如你之前看到的，一些超参数的确要比其它的更重要。
举个例子，假设超参数 1 是𝑎（学习速率），取一个极端的例子，假设超参数 2 是 Adam算法中，分母中的𝜀。在这种情况下，𝑎的取值很重要，而𝜀取值则无关紧要。如果你在网格
中取点，接着，你试验了𝑎的 5 个取值，那你会发现，无论𝜀取何值，结果基本上都是一样的。所以，你知道共有 25 种模型，但进行试验的𝑎值只有 5 个，我认为这是很重要的。
对比而言，如果你随机取值，你会试验 25 个独立的𝑎，似乎你更有可能发现效果做好的那个。

实践中，你搜索的超参数可能不止两个。假如，你有三个超参数，这时你搜索的不是一个方格，而是一个立方体，超参数 3 代表第三维，接着，在
三维立方体中取值，你会试验大量的更多的值，三个超参数中每个都是。

实践中，你搜索的可能不止三个超参数有时很难预知，哪个是最重要的超参数，对于你的具体应用而言，随机取值而不是网格取值表明，你探究了更多重要超参数的潜在值，无论结果是什么。

























