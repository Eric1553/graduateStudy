参考文章：https://blog.csdn.net/m0_37108127/article/details/94571421

4.1 深层神经网络(Deep Neural Networks)
  拥有多个隐藏层的神经网络称为深层神经网络。

  回顾：神经网络层数的定义（layer），从左往右数依次为（0,1,2,3,4……）
  
  如何确定神经网络的层数：尽管对于任何给定的问题很难去提前预测到底需要多深的神经网络，所以先去尝试逻辑回归，尝试一层然后两层隐含层，然后把隐含层的数量看做是另一个可以自由选择大小的超参数，
  然后再保留交叉验证数据上评估，或者用你的开发集来评估。（有一些函数只能由深层神经网络学会）
  
  神经网络中的符号约定：
    输入的特征记为x，由于x是输入层的激活函数，因此x = a[0]
    用L表示神经网络拥有的layer数量，例如L = 4，其中的层数用l代指，本例中l的取值为（0,1,2,3），每一层的激活函数产生的结果记为a[l]
    每一层中使用激活函数g对z[l]计算出结果a[l],其中w[l]表示l层计算z值时的权重，b[l]表示l层计算z[l]值的偏置
    最后一层的激活函数a[L]产生的就是这个神经网络最后预测的输出结果。
  
4.2 前向传播和反向传播（Forward and backward propagation）
  前向传播：
    前向传播输入的参数是a[l-1]，产生的输出时a[l],产生的缓存是z[l](从实现的角度来说我们可以缓存下(𝑤[𝑙]和𝑏[𝑙]，这样更容易在不同的环节中调用函数)
    
    前向传播的步骤:
    𝑧[𝑙] = 𝑊[𝑙] · 𝑎[𝑙−1] + 𝑏[𝑙] · 𝑎[𝑙] = 𝑔[𝑙](𝑧[𝑙])  
    向量化实现过程可以写成： 𝑧[𝑙] = 𝑊[𝑙] · 𝐴[𝑙−1] + 𝑏[𝑙] · 𝐴[𝑙] = 𝑔[𝑙](𝑍[𝑙])
  
  反向传播：
    输入为𝑑𝑎[𝑙]，输出为𝑑𝑎[𝑙−1],𝑑𝑤[𝑙],𝑑𝑏[𝑙]
    所以反向传播的步骤可以写成：
    𝑑𝑧[𝑙] = 𝑑𝑎[𝑙] ∗ 𝑔[𝑙]′(𝑧[𝑙]) 
    𝑑𝑤[𝑙] = 𝑑𝑧[𝑙] ⋅ 𝑎[𝑙−1] 
    𝑑𝑏[𝑙] = 𝑑𝑧[𝑙] 
    𝑑𝑎[𝑙−1] = 𝑤[𝑙]𝑇 ⋅ 𝑑𝑧[𝑙] 
    𝑑𝑧[𝑙] = 𝑤[𝑙+1]𝑇𝑑𝑧[𝑙+1] ⋅ 𝑔[𝑙]′(𝑧[𝑙])
    
  向量化实现过程可以写成：
    𝑑𝑍[𝑙] = 𝑑𝐴[𝑙] ∗ 𝑔[𝑙]′(𝑍[𝑙]) 
    𝑑𝑊[𝑙] = 1/𝑚 * 𝑑𝑍[𝑙] ⋅ 𝐴[𝑙−1]𝑇 
    𝑑𝑏[𝑙] = 1/𝑚 * 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑧[𝑙], 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒) 
    𝑑𝐴[𝑙−1] = 𝑊[𝑙]𝑇. 𝑑𝑍[𝑙]
    
  第一层你可能有一个 ReLU 激活函数，第二层为另一个 ReLU 激活函数，第三层可能是sigmoid 函数（如果你做二分类的话），输出值为，用来计算损失；这样你就可以向后迭代进行反向传播
  求导来求𝑑𝑤[3]，𝑑𝑏[3] ，𝑑𝑤[2] ，𝑑𝑏[2] ，𝑑𝑤[1] ，𝑑𝑏[1]。在计算的时候，缓存会把𝑧[1] 𝑧[2]𝑧[3]传递过来，然后回传𝑑𝑎[2]，𝑑𝑎[1] ，可以用来计算𝑑𝑎[0]，但我们不会使用它，
  这里讲述了一个三层网络的前向和反向传播，还有一个细节没讲就是前向递归——用输入数据来初始化，那么反向递归（使用 Logistic 回归做二分类）————对𝐴[𝑙] 求导。
  
4.3 深层网络中的前向传播（Forward propagation in a Deep Network）
  单样本的情况：
    第一层需要计算𝑧[1] = 𝑤[1]𝑥 + 𝑏[1]，𝑎[1] = 𝑔[1](𝑧[1])（𝑥可以看做𝑎[0]）
    第二层需要计算𝑧[2] = 𝑤[2]𝑎[1] + 𝑏[2]，𝑎[2] = 𝑔[2](𝑧[2])
    以此类推，
    第四层为𝑧[4] = 𝑤[4]𝑎[3] + 𝑏[4]，𝑎[4] = 𝑔[4](𝑧[4])
    前向传播可以归纳为多次迭代𝑧[𝑙] = 𝑤[𝑙]𝑎[𝑙−1] + 𝑏[𝑙]，𝑎[𝑙] = 𝑔[𝑙](𝑧[𝑙])。
    
    𝑍[𝑙] = 𝑊[𝑙] * 𝑎[𝑙−1] + 𝑏[𝑙] ， 𝐴[𝑙] = 𝑔[𝑙](𝑍[𝑙]) (𝐴[0] = 𝑋)
    
4.4 核对矩阵的维数（Getting your matrix dimensions right）
  𝑤的维度是（下一层的维数，前一层的维数），即𝑤[𝑙] (𝑛[𝑙],𝑛[𝑙−1])； 
  𝑏的维度是（下一层的维数，1），即𝑏[𝑙] (𝑛[𝑙], 1)
  𝑧[𝑙],𝑎[𝑙]  (𝑛[𝑙], 1);
  
  向量化后：
  𝑍[𝑙]可以看成由每一个单独的𝑍[𝑙]叠加而得到，𝑍[𝑙] = (𝑧[𝑙][1]，𝑧[𝑙][2]，𝑧[𝑙][3]，…，𝑧[𝑙][𝑚])， 𝑚为训练集大小，所以𝑍[𝑙]的维度不再是(𝑛[𝑙], 1)，而是(𝑛[𝑙], 𝑚)。 
  𝐴[𝑙]：(𝑛[𝑙], 𝑚)，𝐴[0] = 𝑋 = (𝑛[𝑙], 𝑚)
  
4.5 为什么使用深层表示？（Why deep representations?）
  首先，深度网络究竟在计算什么？如果你在建一个人脸识别或是人脸检测系统，深度神经网络所做的事就是，当你输入一张脸部的照片，然后你可以把深度神经网络的第一层，当
成一个特征探测器或者边缘探测器。在这个例子里，我会建一个大概有 20 个隐藏单元的深度神经网络，是怎么针对这张图计算的。隐藏单元就是这些图里这些小方块（第一张大图），
举个例子，这个小方块（第一行第一列）就是一个隐藏单元，它会去找这张照片里“|”边缘的方向。那么这个隐藏单元（第四行第四列），可能是在找（“—”）水平向的边缘在哪里。之
后的课程里，我们会讲专门做这种识别的卷积神经网络，到时候会细讲，为什么小单元是这么表示的。你可以先把神经网络的第一层当作看图，然后去找这张照片的各个边缘。我们可
以把照片里组成边缘的像素们放在一起看，然后它可以把被探测到的边缘组合成面部的不同部分（第二张大图）。比如说，可能有一个神经元会去找眼睛的部分，另外还有别的在找鼻
子的部分，然后把这许多的边缘结合在一起，就可以开始检测人脸的不同部分。最后再把这些部分放在一起，比如鼻子眼睛下巴，就可以识别或是探测不同的人脸（第三张大图）。
  
  这种从简单到复杂的金字塔状表示方法或者组成方法，也可以应用在图像或者人脸识别以外的其他数据上。比如当你想要建一个语音识别系统的时候，需要解决的就是如何可视化
语音，比如你输入一个音频片段，那么神经网络的第一层可能就会去先开始试着探测比较低层次的音频波形的一些特征，比如音调是变高了还是低了，分辨白噪音，咝咝咝的声音，
或者音调，可以选择这些相对程度比较低的波形特征，然后把这些波形组合在一起就能去探测声音的基本单元。在语言学中有个概念叫做音位，比如说单词 ca，c 的发音，“嗑”就是一个
音位，a 的发音“啊”是个音位，t 的发音“特”也是个音位，有了基本的声音单元以后，组合起来，你就能识别音频当中的单词，单词再组合起来就能识别词组，再到完整的句子。

  所以深度神经网络的这许多隐藏层中，较早的前几层能学习一些低层次的简单特征，等到后几层，就能把简单的特征结合起来，去探测更加复杂的东西。比如你录在音频里的单词、
词组或是句子，然后就能运行语音识别了。同时我们所计算的之前的几层，也就是相对简单的输入函数，比如图像单元的边缘什么的。到网络中的深层时，你实际上就能做很多复杂的
事，比如探测面部或是探测单词、短语或是句子。
    
    另外一个，关于神经网络为何有效的理论，来源于电路理论，它和你能够用电路元件计算哪些函数有着分不开的联系。根据不同的基本逻辑门，譬如与门、或门、非门。在非正式
的情况下，这些函数都可以用相对较小，但很深的神经网络来计算，小在这里的意思是隐藏单元的数量相对比较小，但是如果你用浅一些的神经网络计算同样的函数，也就是说在我们
不能用很多隐藏层时，你会需要成指数增长的单元数量才能达到同样的计算结果。

  当我开始解决一个新问题时，我通常会从 logistic 回归开始，再试试一到两个隐层，把隐藏层数量当作参数、超参数一样去调试，这样去找比较合适的深度。
  但是近几年以来，有一些人会趋向于使用非常非常深邃的神经网络，比如好几打的层数，某些问题中只有这种网络才是最佳模型。
    
4.6 搭建神经网络块（Building blocks of deep neural networks）
  
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  
