3.1 神经网络概述（Neural Network Overview）
  什么是神经网络？
    神经网络是由一个个的神经元（节点、感知器）构成的，每个神经元都参与了特定的运算，通过对所有的神经元的运算进行组织，最后得到一个输出。、
  ###  
    如何配置神经网络中的层数与节点：https://www.sohu.com/a/245833938_787107
  ###  
  神经网路的层数？
    神经网络通常有一个输入层，若干个隐藏层，一个输出层。因此从技术上而言神经网络应该包含这三个层次，但是实际在应用中我们只需要考虑隐藏层与输出层的层数，两层之和得到的称为神经网络的层数。
    Tips：输入层通常被称为第零层
  ###
  神经网络是如何运作的？
    神经网络的基本原理：对于一个两层的浅层神经网络，其运行原理为：对隐藏层进行线性计算，线性计算结果z作为自变量代入激活函数得到第一层隐藏层输出a[1] = σ(z) ，
                       此时的a[1]作为输入与第二层的参数w2，b2再进行相同的运算得到最终输出a[2]。最后计算损失函数 L(a[2] , y)

3.2 神经网络的表示（Neural Network Representation）
  输入层：有输入特征𝑥1、𝑥2、𝑥3，它们被竖直地堆叠起来，这叫做神经网络的输入层。它包含了神经网络的输入；然后这里有另外一层我们称之为隐藏层。在本例中最后一层只由一个结点构成，而这个只
  
  输出层：有一个结点的层被称为输出层，它负责产生预测值。
  
  隐藏层：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入𝑥也包含了目标输出𝑦，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，
    也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。
  
3.3 计算一个神经网络的输出（Computing a Neural Network's output）
  假设我们有三个特征的输入，4个隐藏层神经元(一个隐藏层)，一个输出层神经元：
    1.输入特征x1,x2,x3，构成了一个堆叠成(3,1)向量；
    
    2.如何确定权重wi的矩阵（数组）大小呢？首先，矩阵乘法对于wT * x的定义决定了w用来控制输出的行数。可以发现，隐藏层有四个神经元，输入层有三个神经元，因此我们的权重W构成了一个4×3的矩阵
    （即，w矩阵的大小应该是（W所在层的神经元个数，W之前一层的神经元个数））  bias是形状与输出矩阵相同的矩阵，即[4,1]
    
    3.隐藏层对输入进行线性运算、引入激活函数，4×3的W[1]矩阵与3×1的x矩阵运算得到输出为a[1] 其shape即4×1。
    
    4.输出层对隐藏层的输出a[1]进行计算，w[2]矩阵的形状是(1,4)与形状为[4,1]的a[1]矩阵进行运算，得到结果为[1,1]的输出a[2]，这就是整个两层神经网络的输出。
    
    需要注意，下一层的输入是上一层的输出结果，即每次的输入都是经过激活函数计算之后的数值。
    
3.4 多样本向量化（Vectorizing across multiple examples） 
  在引入多样本后，要注意区分上标的记号。例如z[3](2)表示第三层（不计算输入层，即第三个隐藏层）的第二个样本计算出的z值。
  
  在一个两层神经网络中，向量化以后的各个矩阵形式如下（即将m个样本横向堆叠，纵向表示各个样本中包含的参数或者特征）
  公式 3.12： 𝑥 = [𝑥(1) 𝑥(2) ⋯ 𝑥(𝑚)]
  公式 3.13： 𝑍[1] = [ 𝑧[1](1) 𝑧[1](2) ⋯ 𝑧[1](𝑚)]
  公式 3.14： 𝐴[1] = [ 𝛼[1](1) 𝛼[1](2) ⋯ 𝛼[1](𝑚)]
  公式 3.15： 𝑧[1](𝑖) = 𝑊[1](𝑖)𝑥(𝑖) + 𝑏[1]        𝛼[1](𝑖) = 𝜎(𝑧[1](𝑖)) 
              𝑧[2](𝑖) = 𝑊[2](𝑖)𝛼[1](𝑖) + 𝑏[2]     𝛼[2](𝑖) = 𝜎(𝑧[2](𝑖))}
              
              ⟹ 
              
              𝐴[1] = 𝜎(𝑧[1])   𝑧[2] = 𝑊[2]𝐴[1] + 𝑏[2]   𝐴[2] = 𝜎(𝑧[2])
              
  以此类推，从小写的向量𝑥到这个大写的矩阵𝑋，只是通过组合𝑥向量在矩阵的各列中。同理，𝑧[1](1)，𝑧[1](2)等等都是𝑧[1](𝑚)的列向量，将所有𝑚都组合在各列中，就的到矩阵𝑍[1]。
  同理，𝑎[1](1)，𝑎[1](2)，……，𝑎[1](𝑚)将其组合在矩阵各列中，如同从向量𝑥到矩阵𝑋，以及从向量𝑧到矩阵𝑍一样，就能得到矩阵𝐴[1]。
  同样的，对于𝑍[2]和𝐴[2]，也是这样得到。
  
  水平索引与垂直索引所代表的具体含义：水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。
  
3.5 向量化实现的解释 （Justification for vectorized implementation）
  具体的实现方法即： 𝑍[n] = 𝑊[n]𝐴[n-1] + 𝑏[n]

3.6 激活函数（Activation functions）（参见repository中的激活函数）
  结论：
    双曲正切函数的效果总是优于sigmoid函数（sigmoid函数只有在二分类问题中才有较好的效果）。在这个例子中对隐藏层使用 tanh 激活函数，输出层使用 sigmoid 函数（不同层的激活函数可以不同）
     
    ReLU（修正线型单元，Rectified Linear Unit）的应用是一次革新，其变形有Leaky ReLU。
    为什么要引入ReLU？ReLU解决了tanh和sigmoid的共同问题：输入过大或者过小时梯度变化非常慢
      ReLU的数学形式：a = max(0,z) 在实际应用中，0实际上是一个很小的正数。如果z大于零，其导数恒为1
      
    这有一些选择激活函数的经验法则：
      如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会
      使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当𝑧是负值的时候，导数等于 0。

     这里也有另一个版本的 Relu 被称为 Leaky Relu。 当𝑧是负值时，这个函数的值不是等于 0，而是轻微的倾斜。这个函数通常比 Relu 激活函数效果要好，尽管在实际中 Leaky ReLu 使用的并不多。
     多个激活函数的对比：https://blog.csdn.net/qq_23304241/article/details/80300149
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
