3.1 神经网络概述（Neural Network Overview）
  什么是神经网络？
    神经网络是由一个个的神经元（节点、感知器）构成的，每个神经元都参与了特定的运算，通过对所有的神经元的运算进行组织，最后得到一个输出。、
  ###  
    如何配置神经网络中的层数与节点：https://www.sohu.com/a/245833938_787107
  ###  
  神经网路的层数？
    神经网络通常有一个输入层，若干个隐藏层，一个输出层。因此从技术上而言神经网络应该包含这三个层次，但是实际在应用中我们只需要考虑隐藏层与输出层的层数，两层之和得到的称为神经网络的层数。
    Tips：输入层通常被称为第零层
  ###
  神经网络是如何运作的？
    神经网络的基本原理：对于一个两层的浅层神经网络，其运行原理为：对隐藏层进行线性计算，线性计算结果z作为自变量代入激活函数得到第一层隐藏层输出a[1] = σ(z) ，
                       此时的a[1]作为输入与第二层的参数w2，b2再进行相同的运算得到最终输出a[2]。最后计算损失函数 L(a[2] , y)

3.2 神经网络的表示（Neural Network Representation）
  输入层：有输入特征𝑥1、𝑥2、𝑥3，它们被竖直地堆叠起来，这叫做神经网络的输入层。它包含了神经网络的输入；然后这里有另外一层我们称之为隐藏层。在本例中最后一层只由一个结点构成，而这个只
  
  输出层：有一个结点的层被称为输出层，它负责产生预测值。
  
  隐藏层：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入𝑥也包含了目标输出𝑦，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，
    也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。
  
3.3 计算一个神经网络的输出（Computing a Neural Network's output）
  假设我们有三个特征的输入，4个隐藏层神经元(一个隐藏层)，一个输出层神经元：
    1.输入特征x1,x2,x3，构成了一个堆叠成(3,1)向量；
    
    2.如何确定权重wi的矩阵（数组）大小呢？首先，矩阵乘法对于wT * x的定义决定了w用来控制输出的行数。可以发现，隐藏层有四个神经元，输入层有三个神经元，因此我们的权重W构成了一个4×3的矩阵
    （即，w矩阵的大小应该是（W所在层的神经元个数，W之前一层的神经元个数））  bias是形状与输出矩阵相同的矩阵，即[4,1]
    
    3.隐藏层对输入进行线性运算、引入激活函数，4×3的W[1]矩阵与3×1的x矩阵运算得到输出为a[1] 其shape即4×1。
    
    4.输出层对隐藏层的输出a[1]进行计算，w[2]矩阵的形状是(1,4)与形状为[4,1]的a[1]矩阵进行运算，得到结果为[1,1]的输出a[2]，这就是整个两层神经网络的输出。
    
    需要注意，下一层的输入是上一层的输出结果，即每次的输入都是经过激活函数计算之后的数值。
    
3.4 多样本向量化（Vectorizing across multiple examples） 
  在引入多样本后，要注意区分上标的记号。例如z[3](2)表示第三层（不计算输入层，即第三个隐藏层）的第二个样本计算出的z值。
  
  在一个两层神经网络中，向量化以后的各个矩阵形式如下（即将m个样本横向堆叠，纵向表示各个样本中包含的参数或者特征）
  公式 3.12： 𝑥 = [𝑥(1) 𝑥(2) ⋯ 𝑥(𝑚)]
  公式 3.13： 𝑍[1] = [ 𝑧[1](1) 𝑧[1](2) ⋯ 𝑧[1](𝑚)]
  公式 3.14： 𝐴[1] = [ 𝛼[1](1) 𝛼[1](2) ⋯ 𝛼[1](𝑚)]
  公式 3.15： 𝑧[1](𝑖) = 𝑊[1](𝑖)𝑥(𝑖) + 𝑏[1]        𝛼[1](𝑖) = 𝜎(𝑧[1](𝑖)) 
              𝑧[2](𝑖) = 𝑊[2](𝑖)𝛼[1](𝑖) + 𝑏[2]     𝛼[2](𝑖) = 𝜎(𝑧[2](𝑖))}
              
              ⟹ 
              
              𝐴[1] = 𝜎(𝑧[1])   𝑧[2] = 𝑊[2]𝐴[1] + 𝑏[2]   𝐴[2] = 𝜎(𝑧[2])
              
  以此类推，从小写的向量𝑥到这个大写的矩阵𝑋，只是通过组合𝑥向量在矩阵的各列中。同理，𝑧[1](1)，𝑧[1](2)等等都是𝑧[1](𝑚)的列向量，将所有𝑚都组合在各列中，就的到矩阵𝑍[1]。
  同理，𝑎[1](1)，𝑎[1](2)，……，𝑎[1](𝑚)将其组合在矩阵各列中，如同从向量𝑥到矩阵𝑋，以及从向量𝑧到矩阵𝑍一样，就能得到矩阵𝐴[1]。
  同样的，对于𝑍[2]和𝐴[2]，也是这样得到。
  
  水平索引与垂直索引所代表的具体含义：水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。
  
3.5 向量化实现的解释 （Justification for vectorized implementation）
  具体的实现方法即： 𝑍[n] = 𝑊[n]𝐴[n-1] + 𝑏[n]

3.6 激活函数（Activation functions）（参见repository中的激活函数）
  结论：
    双曲正切函数的效果总是优于sigmoid函数（sigmoid函数只有在二分类问题中才有较好的效果）。在这个例子中对隐藏层使用 tanh 激活函数，输出层使用 sigmoid 函数（不同层的激活函数可以不同）
     
    ReLU（修正线型单元，Rectified Linear Unit）的应用是一次革新，其变形有Leaky ReLU。
    为什么要引入ReLU？ReLU解决了tanh和sigmoid的共同问题：输入过大或者过小时梯度变化非常慢
      ReLU的数学形式：a = max(0,z) 在实际应用中，0实际上是一个很小的正数。如果z大于零，其导数恒为1
      
    这有一些选择激活函数的经验法则：
      如果输出是 0、1 值（二分类问题），则输出层选择 sigmoid 函数，然后其它的所有单元都选择 Relu 函数。这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会
      使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当𝑧是负值的时候，导数等于 0。

     这里也有另一个版本的 Relu 被称为 Leaky Relu。 当𝑧是负值时，这个函数的值不是等于 0，而是轻微的倾斜。这个函数通常比 Relu 激活函数效果要好，尽管在实际中 Leaky ReLu 使用的并不多。
     多个激活函数的对比：https://blog.csdn.net/qq_23304241/article/details/80300149
     
   ReLU和Leaky ReLU的优点：
     第一，在𝑧的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，
     在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。

    第二，sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和 Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。
    (同时应该注意到的是，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会有这问题)
    𝑧在 ReLu 的梯度一半都是 0，但是，有足够的隐藏层使得 z 值大于 0，所以对大多数的训练数据来说学习过程仍然可以很快。
    
    快速概括一下不同激活函数的过程和结论。
    sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。
    tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。
    ReLu 激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用 ReLu 或者Leaky ReLu。
    
    公式 3.23： Leaky ReLU:  𝑎 = 𝑚𝑎𝑥(0.01𝑧, 𝑧) 
    为什么常数是 0.01？当然，可以为学习算法选择不同的参数。在选择自己神经网络的激活函数时，有一定的直观感受，在深度学习中的经常遇到一个问题：在编写神经网络的时候，
    会有很多选择：隐藏层单元的个数、激活函数的选择、初始化权值……这些选择想得到一个对比较好的指导原则是挺困难的。
    
    如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。
    为自己的神经网络的应用测试这些不同的选择，会在以后检验自己的神经网络或者评估算法的时候，看到不同的效果。如果仅仅遵守使用默认的 ReLu 激活函数，而不要用其他的
    激励函数，那就可能在近期或者往后，每次解决问题的时候都使用相同的办法。
    
3.7 为什么需要非线性激活函数？（why need a nonlinear activation function?）
  事实证明如果你在隐藏层用线性激活函数，在输出层用 sigmoid 函数，那么这个模型的复杂度和没有任何隐藏层的标准 Logistic 回归是一样的。
  总而言之，不能在隐藏层用线性激活函数，可以用 ReLU 或者 tanh 或者 leaky ReLU 或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会
  在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。 
    
    
3.8 激活函数的导数（Derivatives of activation functions）
    sigmoid函数：设 a = sigmoid(x) = g(x)  dg(x)/dx = a(1-a)
    
    tanh函数：设 a = sigmoid(x) = g(x) dg(x)/dx = (1-tanh(z))²
      注：当𝑧 = 10 或𝑧 = −10 𝑑𝑔(𝑧)/𝑑𝑧≈ 0 
          当𝑧 = 0， 𝑑𝑔(𝑧)/𝑑𝑧= 1 − (0) = 1
    ReLU: 输入自变量值等于0时无定义。大于0导数为1，小于0为0
    Leaky ReLU：输入自变量值等于0时无定义。大于0导数为1，小于0为0.01
    
3.9 神经网络的梯度下降 （Gradient descent for neural networks）& 3.10（选修）直观理解反向传播（Backpropagation intuition）

公式 3.44： 𝑑𝑍[2] = 𝐴[2] − 𝑌 ， 𝑑𝑊[2] = 1𝑚 𝑑𝑍[2]𝐴[1]𝑇
公式 3.45： 𝐿 = (1/𝑚) ∑ 𝐿(𝑦^ , 𝑦) 
公式 3.46： 𝑑𝑏[2] = (1/𝑚) * 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍[2], 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒)
公式 3.47： 𝑑𝑍[1]  = 𝑊[2]𝑇 * 𝑑𝑍[2]∗ 𝑔[1]′(𝑍[1])
          (𝑛[1],𝑚)     (𝑛[1],𝑚)     (𝑛[1],𝑚)
公式 3.48： 𝑑𝑊[1] = (1/𝑚) * 𝑑𝑍[1] * 𝑥𝑇
公式 3.49： 𝑑𝑏[1] = (1/𝑚) * 𝑛𝑝. 𝑠𝑢𝑚(𝑑𝑍[1], 𝑎𝑥𝑖𝑠 = 1, 𝑘𝑒𝑒𝑝𝑑𝑖𝑚𝑠 = 𝑇𝑟𝑢𝑒)

3.11 随机初始化（Random+Initialization）
  本节讨论如何将w和b参数进行正确初始化的过程。
  
  b参数表示一个标量bias，可以初始化为0而不会对网络的输出产生影响。
  
  下面重点讨论如何对w参数进行正确的初始化：
  1.w参数能否初始化为0：
    答案是否定的，w参数不能直接初始化为0，这样会导致wx+b的结果仍然是0（无论输入的特征如何，得到的第零层的输出总是0）这样的后果是每个隐含单元都在计算同样的函数，
    他们是完全对称的。在每次迭代之后，所有的隐藏单元仍然是同一个函数，在进行权重更新后，每个W矩阵的所有位置的元素值都是一样的。所有隐藏单元都是对称的神经网络是毫无用处的。
    （我们需要的是不同的隐藏单元完成不同的函数计算）
    
  2.如何进行w参数的初始化：
  使用随机初始化参数。例如将W[1]设置成np.random.randn(2,2)这样就会生成一个2×2的高斯分布的矩阵，再乘上一比较小的数，如0.01（这个数的选取会在下一单元中讨论），这样就会初始化出很小的随机数。
  b的初始化可以直接初始化为0（b不存在 symmetry breaking problem）。同样的对于W[2]可以使用同样的随机初始化（b[2]初始化为np.zeros((2,1))）
  
  3.为什么初始化时要乘上一个比较小的数:
  根据tanh和sigmoid函数的特性，在z值过大或者过小时，产生的a值很有可能停在sigmoid和tanh函数梯度变化很小的地方，导致迭代速度（学习速度）变慢。
  






















  
  
    
    
    
    
    
    
    
    
    
    
    
