本周内容是神经网络的基础知识，如何遍历训练集中的所有样本，如何处理训练集

神经网络计算的机制：前向暂停与后向传播（forward pause and backward propagation）
逻辑回归 logistic regression

2.1 二分类 binary classification
  二分类即输出标签label为 0/1的题目类型，训练目标：获得一个分类器，输入特征向量，预测输出结果是0还是1
  适用于二分类的算法：logistic regression
  
  RGB空间的颜色识别：
  RGB空间的特征向量是每一张照片中的每一个像素的RGB值，每个值的取值范围为0-255
  每张图片具有三个像素矩阵，输入的向量x的总维度为64*64*3（每张图片64*64像素）
  
  输入向量x的维度（nx，1） nx为总的像素度
  输出结果y 取值为（0，1）
  
  训练集矩阵X的表示：
    训练集矩阵是nx行 m列的矩阵 每一列
    都代表一组输入样本的输入向量数据，共m列（m个输入样本）
    X.shape得到的结果就是（nx，m）
    
2.2 Logistic Regression 逻辑回归
  逻辑回归的假设函数（Hypothesis Fuction）
  
  有关实际值y与预测值^y的解释：^y是表示y = 1这一事件的可能性 ^y = P（y = 1｜x）
  有关参数w的解释：w的维度与特征向量x一致，w是一个维度为（nx，1）的向量
  误差b的解释：b表示bias，是一个表示偏差的实数。

  logistic regression的运作过程：
    矩阵x作为输入向量 经过wT*x+b计算出^y的值。但是这个结果可能不在我们需要的范围之中，因此我们要将
    线性函数的运算结果作为自变量输入输入sigmoid之类的激活函数（或其他非线性函数）之中，从而获得一个介于（0，1）的输出
    
    sigmoid函数会将线性函数计算得到的结果映射到（0，1）中，当z（z = wT * x + b）非常大，获得的值会趋近于1，
    输入值非常小，获得的输出就会无限接近于0
    
2.3 Cost Function 代价函数/成本函数
  误差函数/损失函数 Loss function   L（y，^y）
  逻辑回归的优化目标与凸优化是有区别的
    
  
