本周内容是神经网络的基础知识，如何遍历训练集中的所有样本，如何处理训练集

神经网络计算的机制：前向暂停与后向传播（forward pause and backward propagation）
逻辑回归 logistic regression

2.1 二分类 binary classification
  二分类即输出标签label为 0/1的题目类型，训练目标：获得一个分类器，输入特征向量，预测输出结果是0还是1
  适用于二分类的算法：logistic regression
  
  RGB空间的颜色识别：
  RGB空间的特征向量是每一张照片中的每一个像素的RGB值，每个值的取值范围为0-255
  每张图片具有三个像素矩阵，输入的向量x的总维度为64*64*3（每张图片64*64像素）
  
  输入向量x的维度（nx，1） nx为总的像素数
  输出结果y 取值为（0，1）
  
  训练集矩阵X的表示：
    训练集矩阵是nx行 m列的矩阵 每一列
    都代表一组输入样本的输入向量数据，共m列（m个输入样本）
    X.shape得到的结果就是（nx，m）
    
2.2 Logistic Regression 逻辑回归
  逻辑回归的假设函数（Hypothesis Fuction）
  
  有关实际值y与预测值y^的解释：y^是表示y = 1这一事件的可能性 y^ = P（y = 1｜x）
  有关参数w的解释：w的维度与特征向量x一致，w是一个维度为（nx，1）的向量
  误差b的解释：b表示bias，是一个表示偏差的实数。

  logistic regression的运作过程：
    矩阵x作为输入向量 经过wT*x+b计算出y^的值。但是这个结果可能不在我们需要的范围之中，因此我们要将
    线性函数的运算结果作为自变量输入输入sigmoid之类的激活函数（或其他非线性函数）之中，从而获得一个介于（0，1）的输出
    
    sigmoid函数会将线性函数计算得到的结果映射到（0，1）中，当z（z = wT * x + b）非常大，获得的值会趋近于1，
    输入值非常小，获得的输出就会无限接近于0
    
2.3 Cost Function 代价函数/成本函数
  误差函数/损失函数 Loss function   L（y，y^）
  
  逻辑回归的优化目标与凸优化是有区别的。这个𝐿称为的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者它们平方差的一半，
  但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，
  虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。
  
  逻辑回归中的损失函数通常定义为： 𝐿(𝑦^,𝑦) = −𝑦log(𝑦^) − (1 − 𝑦)log(1 − 𝑦^)
  
  当𝑦=1时损失函数𝐿=−log(𝑦^)，如果想要损失函数𝐿尽可能得小，那么𝑦^就要尽可能大，因为sigmoid函数取值[0,1]，所以𝑦^会无限接近于1。
  当𝑦=0时损失函数𝐿=−log(1 − 𝑦^)，如果想要损失函数𝐿尽可能得小，那么𝑦^就要尽可能小，因为sigmoid函数取值[0,1]，所以𝑦^会无限接近于0。
  定义损失函数的思想：如果𝑦等于1，我们就尽可能让𝑦^变大，如果𝑦等于0，我们就尽可能让𝑦^变小。
  
  为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对𝑚个样本的损失函数求和然后除以𝑚: 
                  𝐽(𝑤,𝑏) = (1/𝑚)∑𝐿(𝑦^(𝑖),𝑦(𝑖)) = (1/𝑚)∑(−𝑦(𝑖)log𝑦^(𝑖) − (1 − 𝑦(𝑖))log(1 − 𝑦^(𝑖)))  (求代价函数的平均值)
  损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的𝑤和𝑏，来让代价函数𝐽的总代价降到最低。
  
2.4 梯度下降法（Gradient Descent）
  通过最小化的代价函数J(w,b)来训练参数w和b
  
  recap（摘要重述）：y^ = σ（wT * x + b） σ（x） =  1 / (1 + exp（-x）)
  Cost function:  𝐽(𝑤,𝑏) = (1/𝑚)∑𝐿(𝑦^(𝑖),𝑦(𝑖)) = (1/𝑚)∑(−𝑦(𝑖)log𝑦^(𝑖) − (1 − 𝑦(𝑖))log(1 − 𝑦^(𝑖)))
  
  思路：使用sigmoid函数来对产生的预测值进行输出，将输出结果映射到（0，1）区间上，然后通过Loss function 的最小值来确定梯度下降的终止条件，训练出w和b
  
  梯度下降的过程：
  1.随机初始化w，b在logistic regression中是不会对结果产生较大影响的，迭代最后都会在大致相似的位置终止。
   
  2.迭代过程：梯度向最陡的方向下降进行迭代，重复这个过程。
   
  3.接近全局最优解或者达到全局最优解的时候迭代终止。
  
  w参数进行迭代的过程：𝑤 ≔ 𝑤 − 𝑎 * 𝜕𝐽(𝑤,𝑏)/𝜕𝑤  
  b参数进行迭代的过程：𝑏 ≔ 𝑏 − 𝑎 * 𝜕𝐽(𝑤,𝑏)/𝜕𝑏     (这里的a就是learning rate学习率，用来控制梯度下降的速率，或者称步长step)
   
2.5-2.8 导数与计算图
  掌握导数的计算法则以及在复合函数的情况下求导数的过程（链式运算）
  
2.9 Logistic Regression Gradient Descent
   逻辑回归的公式定义如下：𝑦^ = 𝜎(𝑧) 其中 𝑧 = 𝑤𝑇 * 𝑥 + 𝑏，𝜎(𝑧) = 1/(1 + exp(-x))
   损失函数：𝐿(𝑦^(𝑖),𝑦(𝑖)) = −𝑦(𝑖)log𝑦^(𝑖) − (1 − 𝑦(𝑖))log(1 − 𝑦^(𝑖))
   代价函数：𝐽(𝑤,𝑏) = (1/𝑚)∑𝐿(𝑦^(𝑖),𝑦(𝑖))
   
   假设现在只考虑单个样本的情况，单个样本的代价函数定义如下：𝐿(𝑎,𝑦) = −(𝑦log(𝑎) + (1 − 𝑦)log(1 − 𝑎))  其中𝑎是逻辑回归的输出，𝑦是样本的标签值。
   1. 𝑤和𝑏的修正量可以表达如下：𝑤 = 𝑤 − lr * 𝜕𝐽(𝑤,𝑏)/𝜕𝑤，𝑏= 𝑏 − lr * 𝜕𝐽(𝑤,𝑏)/𝜕𝑏
   2. 计算：𝑦^ = 𝑎 = 𝜎(𝑧)也就是计算图的下一步。
   3. 最后计算损失函数𝐿(𝑎,𝑦)。

   具体计算过程（注意sigmoid函数derivative的性质 即 d(f(x))/d(x) = f(x)(1 - f(x)))
   假设样本只有两个特征𝑥1和𝑥2，为了计算𝑧，我们需要输入参数𝑤1、𝑤2和𝑏，除此之外还有特征值𝑥1和𝑥2。因此𝑧的计算公式为：𝑧 = 𝑤1𝑥1 + 𝑤2𝑥2 + 𝑏
   使用dz来代指对z求导。
      首先，初始化w,b。计算出由第一组参数w,b得到的y^，即a；计算Loss function L(a,y)。通过反向传播的性质来计算dL/dz,使用sigmoid函数的求导性质得到dL/dz = dL/dσ * dσ/dz =  a - y
      求出三个参数：dw1,dw2,db  
        dw1 = (1/m)Σx1 * dz
        dw2 = (1/m)Σx2 * dz
        db = (1/m)Σdz
        
      因此，关于单个样本的梯度下降算法，你所需要做的就是如下的事情：
      使用公式𝑑𝑧 = (𝑎 − 𝑦)计算𝑑𝑧，使用𝑑𝑤1 = 𝑥1 * 𝑑𝑧计算𝑑𝑤1，𝑑𝑤2 = 𝑥2 * 𝑑𝑧计算𝑑𝑤2，𝑑𝑏 = 𝑑𝑧来计算𝑑𝑏，
      然后: 更新𝑤1=𝑤1 − lr * 𝑑𝑤1，更新𝑤2 = 𝑤2 − lr * 𝑑𝑤2，更新𝑏 = 𝑏 − lr * 𝑑𝑏
   
2.10 m 个样本的梯度下降(Gradient Descent on m Examples)
  在单个样本的梯度下降中，每次迭代都乘上一个1/m就可以无需再最后再对每个参数进行平均，这样在一次遍历m个样本的过程后可以直接对w和b的值进行修改。
  但一次迭代只是一次对w和b的值进行修改的过程，我们还需要设置另一个for循环（如果不适用pytorch或者np中的数学方法的前提下）来对所有的特征进行一次遍历，从而确定迭代的终止条件
    迭代的终止条件可以是Cost function的变化小于一个很小的值，表明基本达到了一个全局最优点；或者直接规定iteration的次数从而得到最后迭代的结果。
  
  每次迭代的过程：
    在遍历m个样本之前，首先定义初始值J,dw1,dw2,db
    在每次对m个样本进行遍历的过程中，首先定义好z(i) = wT * x(i) + b 然后根据z计算出a(代入sigmoid函数中)
    得到a以后我们就能写出具体的J的表达式，然后计算dz,dw1,dw2,db，一次遍历m个样本的循环就结束了
    跳出小循环以后，在大循环中对w与b的值进行修改，从而完成一次迭代。
    
  但是使用for循环是十分费时并且没有效率的，因此我们需要用到其他的方法。

2.11 向量化(vectorization)
  向量化是很好的去除for循环的基础方法。
  for循环是非向量化实现矩阵相乘的方法，十分费时。
  
  Vectorization是将两个向量直接进行运算的方法，例如numpy中返回矩阵对应位置相乘结果的np.dot()方法。比起普通的for循环，np.dot()的运行速度大幅领先。
  因此，经验法则是：不适用for循环来对整个数据集进行遍历。
  
  深度学习数据集利用GPU（CPU，CPU与GPU并行）进行SIMD计算可以大幅加快运算的速度。
  CPU，CPU和GPU都有并行化的指令，他们有时候会叫做SIMD指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了built-in函数,像np.function或者并不要求你实现循环的
  函数，它可以让python的充分利用并行化计算。事实在GPU和CPU上面计算，GPU更加擅长SIMD计算。
  
2.12 向量化的更多例子
  将向量u中的每个值转化为向量v中每个值的以e为底的指数形式。首先用np.zeros(dim)初始化一个形状为dim的矩阵(例如参数为（3，4）得到的就是一个三行四列的零矩阵)
  使用u = np.exp(v)就可以实现相应的功能。
  
  其他的numpy数学方法 np.log() np.abs() np.maximun 或者直接对v进行操作：v ** 2 v = 1/v等都可以对向量进行变换。
  
  回到对于Gradient Descent的问题上，我们可以优化的部分就是对于多个参数dw1,dw2,dw3…………的循环，我们可以定义一个shape为(nx,1)的向量来存储对应的dwi的值，其中的nx就是样本的特征数
  这样使用np.zeros((nx,1))来统一初始化dwi，然后使用矩阵的加法来对dw进行计算，这样对应位置的值就是dwi的值(dw += x(i)dz(i))，使用这个方法就减少了一层循环次数。
  
2.13 向量化的逻辑回归 Vertorizing Logistic Regression
  使用向量化的方法来对逻辑回归重新组织。
  回顾：我们拥有m个样本，每个样本都拥有nx个特征值。首先我们对m个样本计算出z(1)，z(2)……z(m)，然后通过前向传播来计算出各组w和b的值，最后进行迭代。
  
  如何不使用for循环来实现这个计算过程？
  ###
  首先，构建一个(nx,m)的数组来代表所有样本的所有特征矩阵，每个样本的特征值按列进行排列。
  ###
  如何计算m个z(i)值呢，首先明确z(i)是一个（1，m）的矩阵，计算z(i)需要计算wT * x + b 注意w的转置得到的是一个（1，m）的矩阵，x是相同形状的矩阵，使用np.dot(wT,x)得到(1,m)的矩阵
  最后再加上一个标量bias 即 z = np.dot(wT,x) + b   其中b是一个(1,1)的矩阵，这里应用了广播的机制使得b能够被正确加到矩阵中的每个元素上。
  ###
  接着需要计算a(i),同样将a(i)向量横向叠加，a(i) = sigmoid(z(i))，这样就获得了跟z(i).shape 相同形状的(1,m)矩阵
  
2.14 向量化的logistic descent的梯度输出
  如何同时计算m个样本的梯度？
  有sigmoid函数的性质，我们可以知道sigmoid的derivative的结果是f(x)(1 - f(x)) = a - y，也就是说我们下一步需要找到一个m维的行向量（与a这个m维行向量形状相同）来与a做减法得到dz
  接下来就是计算dw与db：
    db在原来代码中的计算过程为 db = (1/m) * Σdz 因此在向量化之后，只需要(1/m) * np.sum(dZ)就可以计算   (dZ表示一个m维行向量，即所有dz横向排列的矩阵)   
    dw的计算与db类似，其在原来代码中的计算方法为 dw = (1/m) * x * dzT 即𝑑𝑤 = (1/𝑚) ∗ (𝑥(1)𝑑𝑧(1) + 𝑥(2)𝑑𝑧(2) + ... + 𝑥(𝑚)𝑑𝑧(𝑚)) 注意这里的T转置使得dw又变回m维的列向量。
    
    ！！！前五行代码完成前向传播与后向传播,后两行完成了参数的更新：（一次迭代所完成的参数更新过程）
    𝑍 = 𝑤𝑇 * 𝑋 + 𝑏 = 𝑛𝑝.𝑑𝑜𝑡(𝑤.𝑇,𝑋) + 𝑏 
    𝐴 = 𝜎(𝑍)
    𝑑𝑍 = 𝐴 − 𝑌 
    𝑑𝑤 = 1/𝑚 ∗ 𝑋 ∗ 𝑑𝑧𝑇
    𝑑𝑏 = 1/𝑚 ∗ 𝑛𝑝.𝑠𝑢𝑚(𝑑𝑍)
    𝑤 := 𝑤 −lr ∗ 𝑑𝑤 
    𝑏 := 𝑏 − lr ∗ 𝑑𝑏
    
2.15 Broadcast python中的广播技术
  广播技术可以方便矩阵的运算，不需要每次都对矩阵进行重组操作。
  numpy广播机制：如果两个数组的后缘维度的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为1的维度上进行。
  理解：如果满足两个数组（矩阵）的后缘维度的轴长度相符（可以用matrixA.shape[-1]获取最后一个维度的轴长度）湖畔这其中一个的后缘维度的轴长度为1，就可以进行广播。
  广播操作即将两个数组中的某个扩容成相同shape的数组，从而进行相应的矩阵运算。（具体可以参见广播机制讲解的png）
    
    
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
    
   
   
   
   
   
   
